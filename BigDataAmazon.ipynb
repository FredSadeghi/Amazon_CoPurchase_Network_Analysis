{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FredSadeghi/Amazon_CoPurchase_Network_Analysis/blob/main/BigDataAmazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K1oj8EP5ZgpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6e81cc-1a7f-4240-aabd-4e06d91f72a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: python-louvain 0.16\n",
            "Uninstalling python-louvain-0.16:\n",
            "  Successfully uninstalled python-louvain-0.16\n",
            "\u001b[33mWARNING: Skipping community as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting python-louvain\n",
            "  Using cached python_louvain-0.16-py3-none-any.whl\n",
            "Requirement already satisfied: leidenalg in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: python-igraph in /usr/local/lib/python3.11/dist-packages (0.11.8)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from python-louvain) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from python-louvain) (2.0.2)\n",
            "Requirement already satisfied: igraph<0.12,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from leidenalg) (0.11.8)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from igraph<0.12,>=0.10.0->leidenalg) (1.7.0)\n",
            "Installing collected packages: python-louvain\n",
            "Successfully installed python-louvain-0.16\n",
            "Successfully imported python-louvain for community detection.\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import igraph as ig\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Install required packages\n",
        "!pip uninstall -y python-louvain community\n",
        "!pip install python-louvain leidenalg python-igraph\n",
        "\n",
        "# Try importing the Louvain method\n",
        "try:\n",
        "    from community import best_partition as louvain_best_partition\n",
        "    USE_LOUVAIN = True\n",
        "    print(\"Successfully imported python-louvain for community detection.\")\n",
        "except ImportError as e:\n",
        "    print(f\"Failed to import python-louvain: {e}\")\n",
        "    print(\"Falling back to NetworkX's greedy_modularity_communities for community detection.\")\n",
        "    USE_LOUVAIN = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/Amazon_CoPurchase_Network_Analysis'):\n",
        "    !git clone https://github.com/FredSadeghi/Amazon_CoPurchase_Network_Analysis.git\n",
        "\n",
        "else:\n",
        "    print(\"Repository already cloned. Skipping.\")"
      ],
      "metadata": {
        "id": "ZNa-1qgy9J5m",
        "outputId": "2394843f-908d-41b4-8d21-48a08abb63fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already cloned. Skipping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 1: Preprocessing with Caching ---\n",
        "# Input and output file paths\n",
        "input_file = '/content/Amazon_CoPurchase_Network_Analysis/amazon-meta.txt.gz'\n",
        "product_output = 'products_cleaned.csv'\n",
        "category_output = 'categories_cleaned.csv'\n",
        "review_output = 'reviews_cleaned.csv'\n",
        "edge_output = 'edges.csv'\n",
        "products_enriched_file = 'products_enriched.csv'\n",
        "categories_expanded_file = 'categories_expanded.csv'\n",
        "reviews_processed_file = 'reviews_processed.csv'"
      ],
      "metadata": {
        "id": "s1GXTDjVZxOw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_amazon_data():\n",
        "    \"\"\"Parse raw Amazon metadata into separate CSV files for products, categories, reviews, and weighted edges.\n",
        "    Exclude products with missing, empty, or 'Unknown' Group values.\n",
        "    Only include edges between valid products.\"\"\"\n",
        "    with gzip.open(input_file, 'rt', encoding='latin-1') as f, \\\n",
        "         open(product_output, 'w', newline='', encoding='utf-8') as prod_out, \\\n",
        "         open(category_output, 'w', newline='', encoding='utf-8') as cat_out, \\\n",
        "         open(review_output, 'w', newline='', encoding='utf-8') as rev_out, \\\n",
        "         open(edge_output, 'w', newline='', encoding='utf-8') as edge_out:\n",
        "\n",
        "        product_writer = csv.writer(prod_out)\n",
        "        category_writer = csv.writer(cat_out)\n",
        "        review_writer = csv.writer(rev_out)\n",
        "        edge_writer = csv.writer(edge_out)\n",
        "\n",
        "        # Write headers\n",
        "        product_writer.writerow(['Id', 'ASIN', 'Title', 'Group', 'SalesRank'])\n",
        "        category_writer.writerow(['ASIN', 'CategoryPath'])\n",
        "        review_writer.writerow(['ASIN', 'CustomerID', 'Rating', 'Votes', 'Helpful', 'Sentiment', 'Date'])\n",
        "        edge_writer.writerow(['SourceASIN', 'TargetASIN', 'Weight'])\n",
        "\n",
        "        current = {}\n",
        "        edge_counts = {}  # To track weights for edges\n",
        "        valid_asins = set()  # Track ASINs of products with valid Group and Title\n",
        "        skipped_products = 0\n",
        "        skipped_reviews = 0\n",
        "        skipped_categories = 0\n",
        "        skipped_edges = 0\n",
        "\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            # New product entry\n",
        "            if line.startswith(\"Id:\"):\n",
        "                if current.get('ASIN') and current.get('Id'):\n",
        "                    # Check if group is present and valid\n",
        "                    group = current.get('group')\n",
        "                    title = current.get('title')\n",
        "                    if (group is None or str(group).strip().lower() == 'unknown' or not str(group).strip() or\n",
        "                        title is None or str(title).strip().lower() == 'unknown' or not str(title).strip()):\n",
        "                        skipped_products += 1\n",
        "                    else:\n",
        "                        product_row = [\n",
        "                            current.get('Id'),\n",
        "                            current.get('ASIN'),\n",
        "                            title,\n",
        "                            group,\n",
        "                            current.get('salesrank', '-1')\n",
        "                        ]\n",
        "                        if None in product_row[:2] or any(str(x).strip() == '' for x in product_row[:2]):\n",
        "                            skipped_products += 1\n",
        "                        else:\n",
        "                            product_writer.writerow(product_row)\n",
        "                            valid_asins.add(current['ASIN'])\n",
        "\n",
        "                            for cat in current.get('categories', []):\n",
        "                                if cat and str(cat).strip():\n",
        "                                    category_writer.writerow([current['ASIN'], cat])\n",
        "                                else:\n",
        "                                    skipped_categories += 1\n",
        "\n",
        "                            for review in current.get('reviews', []):\n",
        "                                sentiment = compute_sentiment(review['rating'])\n",
        "                                review_row = [\n",
        "                                    current['ASIN'],\n",
        "                                    review['customer'],\n",
        "                                    review['rating'],\n",
        "                                    review['votes'],\n",
        "                                    review['helpful'],\n",
        "                                    sentiment,\n",
        "                                    review['date']\n",
        "                                ]\n",
        "                                if None in review_row or any(str(x).strip() == '' for x in review_row[:5]):\n",
        "                                    skipped_reviews += 1\n",
        "                                else:\n",
        "                                    review_writer.writerow(review_row)\n",
        "\n",
        "                            for similar_asin in current.get('similar', []):\n",
        "                                if similar_asin and str(similar_asin).strip():\n",
        "                                    edge = tuple(sorted([current['ASIN'], similar_asin]))\n",
        "                                    edge_counts[edge] = edge_counts.get(edge, 0) + 1\n",
        "                                else:\n",
        "                                    skipped_edges += 1\n",
        "\n",
        "                current = {'categories': [], 'reviews': [], 'similar': []}\n",
        "                current['Id'] = line.split('Id:')[1].strip()\n",
        "\n",
        "            elif line.startswith(\"ASIN:\"):\n",
        "                current['ASIN'] = line.split(\"ASIN:\")[1].strip()\n",
        "\n",
        "            elif 'title:' in line:\n",
        "                match = re.search(r'title:\\s*(.*)', line)\n",
        "                if match:\n",
        "                    current['title'] = match.group(1).strip()\n",
        "\n",
        "            elif 'group:' in line:\n",
        "                match = re.search(r'group:\\s*(.*)', line)\n",
        "                if match:\n",
        "                    current['group'] = match.group(1).strip()\n",
        "\n",
        "            elif 'salesrank:' in line:\n",
        "                match = re.search(r'salesrank:\\s*(.*)', line)\n",
        "                if match:\n",
        "                    current['salesrank'] = match.group(1).strip()\n",
        "\n",
        "            elif line.startswith(\"similar:\"):\n",
        "                parts = line.split()\n",
        "                current['similar'] = parts[2:] if len(parts) > 2 else []\n",
        "\n",
        "            elif line.startswith(\"|\"):\n",
        "                current['categories'].append(line.strip())\n",
        "\n",
        "            elif re.match(r'\\d{4}-\\d{1,2}-\\d{1,2}', line):\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 7:\n",
        "                    review = {\n",
        "                        'date': parts[0],  # e.g., 2000-7-28\n",
        "                        'customer': parts[2],\n",
        "                        'rating': int(parts[4]),\n",
        "                        'votes': int(parts[6]),\n",
        "                        'helpful': int(parts[8])\n",
        "                    }\n",
        "                    current['reviews'].append(review)\n",
        "\n",
        "        # Write the last product\n",
        "        if current.get('ASIN') and current.get('Id'):\n",
        "            group = current.get('group')\n",
        "            title = current.get('title')\n",
        "            if (group is None or str(group).strip().lower() == 'unknown' or not str(group).strip() or\n",
        "                title is None or str(title).strip().lower() == 'unknown' or not str(title).strip()):\n",
        "                skipped_products += 1\n",
        "            else:\n",
        "                product_row = [\n",
        "                    current.get('Id'),\n",
        "                    current.get('ASIN'),\n",
        "                    title,\n",
        "                    group,\n",
        "                    current.get('salesrank', '-1')\n",
        "                ]\n",
        "                if None in product_row[:2] or any(str(x).strip() == '' for x in product_row[:2]):\n",
        "                    skipped_products += 1\n",
        "                else:\n",
        "                    product_writer.writerow(product_row)\n",
        "                    valid_asins.add(current['ASIN'])\n",
        "\n",
        "                    for cat in current.get('categories', []):\n",
        "                        if cat and str(cat).strip():\n",
        "                            category_writer.writerow([current['ASIN'], cat])\n",
        "                        else:\n",
        "                            skipped_categories += 1\n",
        "\n",
        "                    for review in current.get('reviews', []):\n",
        "                        sentiment = compute_sentiment(review['rating'])\n",
        "                        review_row = [\n",
        "                            current['ASIN'],\n",
        "                            review['customer'],\n",
        "                            review['rating'],\n",
        "                            review['votes'],\n",
        "                            review['helpful'],\n",
        "                            sentiment,\n",
        "                            review['date']\n",
        "                        ]\n",
        "                        if None in review_row or any(str(x).strip() == '' for x in review_row[:5]):\n",
        "                            skipped_reviews += 1\n",
        "                        else:\n",
        "                            review_writer.writerow(review_row)\n",
        "\n",
        "                    for similar_asin in current.get('similar', []):\n",
        "                        if similar_asin and str(similar_asin).strip():\n",
        "                            edge = tuple(sorted([current['ASIN'], similar_asin]))\n",
        "                            edge_counts[edge] = edge_counts.get(edge, 0) + 1\n",
        "                        else:\n",
        "                            skipped_edges += 1\n",
        "\n",
        "        # Write edges only for valid ASINs\n",
        "        for (source, target), weight in edge_counts.items():\n",
        "            if source in valid_asins and target in valid_asins:\n",
        "                edge_writer.writerow([source, target, weight])\n",
        "            else:\n",
        "                skipped_edges += 1\n",
        "\n",
        "        # Print summary of skipped rows\n",
        "        print(f\"Skipped {skipped_products} product rows due to missing required columns or invalid Group/Title.\")\n",
        "        print(f\"Skipped {skipped_categories} category rows due to missing or empty category paths.\")\n",
        "        print(f\"Skipped {skipped_reviews} review rows due to missing required columns.\")\n",
        "        print(f\"Skipped {skipped_edges} edge rows due to missing/empty ASINs or invalid products.\")"
      ],
      "metadata": {
        "id": "4Y5iTTMiZ8-q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sentiment(rating):\n",
        "    \"\"\"Compute a placeholder sentiment score based on rating (no review text available).\"\"\"\n",
        "    if rating <= 2:\n",
        "        return -1.0  # Negative\n",
        "    elif rating == 3:\n",
        "        return 0.0   # Neutral\n",
        "    else:\n",
        "        return 1.0   # Positive"
      ],
      "metadata": {
        "id": "NM22QwiGaC2-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_data():\n",
        "    \"\"\"Convert CSV files into cleaned, structured pandas DataFrames.\"\"\"\n",
        "    products_df = pd.read_csv(product_output)\n",
        "    categories_df = pd.read_csv(category_output)\n",
        "    reviews_df = pd.read_csv(review_output)\n",
        "\n",
        "    # Drop rows with missing values in critical columns\n",
        "    products_df = products_df.dropna(subset=['Id', 'ASIN'])\n",
        "    categories_df = categories_df.dropna(subset=['ASIN', 'CategoryPath'])\n",
        "    reviews_df = reviews_df.dropna(subset=['ASIN', 'CustomerID', 'Rating', 'Votes', 'Helpful'])\n",
        "\n",
        "    # Double-check filtering for Group and Title\n",
        "    initial_count = len(products_df)\n",
        "    products_df = products_df[\n",
        "        (products_df['Group'].notna()) &\n",
        "        (products_df['Group'].str.strip().str.lower() != 'unknown') &\n",
        "        (products_df['Group'].str.strip() != '') &\n",
        "        (products_df['Title'].notna()) &\n",
        "        (products_df['Title'].str.strip().str.lower() != 'unknown') &\n",
        "        (products_df['Title'].str.strip() != '')\n",
        "    ]\n",
        "    print(f\"Filtered out {initial_count - len(products_df)} product rows with invalid Group or Title in convert_data.\")\n",
        "\n",
        "    # Clean products DataFrame\n",
        "    products_df['SalesRank'] = pd.to_numeric(products_df['SalesRank'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "    # Parse categories\n",
        "    def parse_category_path(cat_path):\n",
        "        if pd.isna(cat_path):\n",
        "            return []\n",
        "        parts = cat_path.split(\"|\")\n",
        "        return [re.sub(r\"\\[\\d+\\]\", \"\", part).strip() for part in parts if part]\n",
        "\n",
        "    categories_df['CategoryLevels'] = categories_df['CategoryPath'].apply(parse_category_path)\n",
        "    categories_expanded = categories_df.explode('CategoryLevels')\n",
        "\n",
        "    # Aggregate review metrics\n",
        "    review_summary = reviews_df.groupby('ASIN').agg({\n",
        "        'CustomerID': 'count',\n",
        "        'Rating': 'mean',\n",
        "        'Votes': 'sum',\n",
        "        'Helpful': 'sum',\n",
        "        'Sentiment': 'mean'\n",
        "    }).rename(columns={\n",
        "        'CustomerID': 'NumReviews',\n",
        "        'Rating': 'AvgRating',\n",
        "        'Votes': 'TotalVotes',\n",
        "        'Helpful': 'TotalHelpful',\n",
        "        'Sentiment': 'AvgSentiment'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Join with products\n",
        "    products_enriched = products_df.merge(review_summary, on='ASIN', how='left')\n",
        "    products_enriched = products_enriched.fillna({\n",
        "        'NumReviews': 0, 'AvgRating': 0.0, 'TotalVotes': 0, 'TotalHelpful': 0, 'AvgSentiment': 0.0\n",
        "    })\n",
        "\n",
        "    # Save final cleaned data\n",
        "    products_enriched.to_csv(products_enriched_file, index=False)\n",
        "    categories_expanded.to_csv(categories_expanded_file, index=False)\n",
        "    reviews_df.to_csv(reviews_processed_file, index=False)\n",
        "\n",
        "    return products_enriched, categories_expanded, reviews_df"
      ],
      "metadata": {
        "id": "Ikzj-sKiaDhz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_with_cache(force_reprocess=False):\n",
        "    \"\"\"Preprocess the data with caching. If cached files exist, load them; otherwise, preprocess.\"\"\"\n",
        "    required_files = [\n",
        "        product_output, category_output, review_output, edge_output,\n",
        "        products_enriched_file, categories_expanded_file, reviews_processed_file\n",
        "    ]\n",
        "\n",
        "    all_files_exist = all(os.path.exists(f) for f in required_files)\n",
        "\n",
        "    if all_files_exist and not force_reprocess:\n",
        "        print(\"Cached preprocessed files found. Loading from cache...\")\n",
        "        start = time.time()\n",
        "        products_enriched = pd.read_csv(products_enriched_file)\n",
        "        categories_expanded = pd.read_csv(categories_expanded_file)\n",
        "        reviews_processed = pd.read_csv(reviews_processed_file)\n",
        "        print(f\"Loading cached data took {time.time() - start:.2f} seconds\")\n",
        "        return products_enriched, categories_expanded, reviews_processed\n",
        "\n",
        "    print(\"Cached files not found or reprocessing forced. Running preprocessing...\")\n",
        "    start = time.time()\n",
        "    print(\"Parsing Amazon metadata...\")\n",
        "    parse_start = time.time()\n",
        "    parse_amazon_data()\n",
        "    print(f\"Parsing took {time.time() - parse_start:.2f} seconds\")\n",
        "\n",
        "    print(\"Converting and cleaning data...\")\n",
        "    convert_start = time.time()\n",
        "    products_enriched, categories_expanded, reviews_processed = convert_data()\n",
        "    print(f\"Converting took {time.time() - convert_start:.2f} seconds\")\n",
        "    print(\"Done. Cleaned data saved to products_enriched.csv, categories_expanded.csv, reviews_processed.csv, and edges.csv\")\n",
        "    print(f\"Total preprocessing took {time.time() - start:.2f} seconds\")\n",
        "    return products_enriched, categories_expanded, reviews_processed"
      ],
      "metadata": {
        "id": "BLCbfZ7MCozJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 2: Graph Construction & Analysis ---\n",
        "# Output file paths\n",
        "edges_file = 'edges.csv'\n",
        "products_file = 'products_enriched.csv'\n",
        "graph_metrics_file = 'graph_metrics.csv'\n",
        "influential_nodes_file = 'influential_nodes.csv'\n",
        "degree_dist_plot = 'degree_distribution.png'\n",
        "graph_plot = 'copurchase_graph.png'"
      ],
      "metadata": {
        "id": "EIbMbIHtQXnM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_copurchase_graph(edges_df, products_df=None):\n",
        "    \"\"\"Construct an undirected graph with weighted edges from co-purchasing relationships.\n",
        "    Only include edges between ASINs present in products_df if provided.\"\"\"\n",
        "    G = nx.Graph()\n",
        "    valid_asins = set(products_df['ASIN']) if products_df is not None else None\n",
        "\n",
        "    edges = []\n",
        "    for _, row in edges_df.iterrows():\n",
        "        source = row['SourceASIN']\n",
        "        target = row['TargetASIN']\n",
        "        if valid_asins is None or (source in valid_asins and target in valid_asins):\n",
        "            edges.append((source, target, {'weight': row['Weight']}))\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    G.add_edges_from(edges)\n",
        "    print(f\"Graph constructed with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
        "    return G"
      ],
      "metadata": {
        "id": "kocCgdySQjCL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_node_attributes(G, products_df):\n",
        "    \"\"\"Add product attributes to graph nodes efficiently.\"\"\"\n",
        "    if products_df is not None:\n",
        "        # Only include nodes that are in products_df\n",
        "        products_df = products_df[products_df['ASIN'].isin(G.nodes)]\n",
        "        attributes = {}\n",
        "        for _, row in products_df.iterrows():\n",
        "            asin = row['ASIN']\n",
        "            attributes[asin] = {\n",
        "                'Title': row['Title'],\n",
        "                'Group': row['Group'],\n",
        "                'SalesRank': row.get('SalesRank', -1),\n",
        "                'AvgRating': row.get('AvgRating', 0.0),\n",
        "                'NumReviews': row.get('NumReviews', 0),\n",
        "                'AvgSentiment': row.get('AvgSentiment', 0.0)\n",
        "            }\n",
        "        nx.set_node_attributes(G, attributes)\n",
        "\n",
        "        # Remove nodes that don't have attributes (shouldn't happen with proper filtering)\n",
        "        nodes_to_remove = [node for node in G.nodes if node not in attributes]\n",
        "        if nodes_to_remove:\n",
        "            print(f\"Removing {len(nodes_to_remove)} nodes that lack attributes (not in products_df).\")\n",
        "            G.remove_nodes_from(nodes_to_remove)"
      ],
      "metadata": {
        "id": "IOhweSq_Qls4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_graph_structure(G):\n",
        "    \"\"\"Analyze the graph's structure and compute basic metrics.\"\"\"\n",
        "    sampled_nodes = list(G.nodes)[:1000]\n",
        "    sampled_graph = G.subgraph(sampled_nodes)\n",
        "\n",
        "    metrics = {\n",
        "        'num_nodes': G.number_of_nodes(),\n",
        "        'num_edges': G.number_of_edges(),\n",
        "        'avg_clustering': nx.average_clustering(sampled_graph),\n",
        "        'num_components': nx.number_connected_components(G)\n",
        "    }\n",
        "    degrees = [d for _, d in G.degree()]\n",
        "    metrics['avg_degree'] = np.mean(degrees) if degrees else 0\n",
        "\n",
        "    print(\"Graph Metrics:\")\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    return metrics, degrees"
      ],
      "metadata": {
        "id": "UguH1UuKQuoY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_degree_distribution(degrees):\n",
        "    \"\"\"Plot the degree distribution with a power-law fit.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    counts = Counter(degrees)\n",
        "    plt.scatter(counts.keys(), counts.values(), color='blue', alpha=0.5, label='Degree')\n",
        "    degrees = np.array(list(counts.keys()))\n",
        "    frequencies = np.array(list(counts.values()))\n",
        "    mask = (degrees > 0) & (frequencies > 0)\n",
        "    log_degrees = np.log10(degrees[mask])\n",
        "    log_freq = np.log10(frequencies[mask])\n",
        "    if len(log_degrees) > 1:\n",
        "        coeffs = np.polyfit(log_degrees, log_freq, 1)\n",
        "        plt.plot(degrees, 10**(coeffs[1] + coeffs[0] * np.log10(degrees)), 'k--',\n",
        "                 label=f'Power-law fit (γ={-coeffs[0]:.2f})')\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Degree')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Degree Distribution of Co-Purchasing Network')\n",
        "    plt.legend()\n",
        "    plt.savefig(degree_dist_plot)\n",
        "    plt.close()\n",
        "    print(f\"Degree distribution plot saved to {degree_dist_plot}\")"
      ],
      "metadata": {
        "id": "-AZ5jTyJQw58"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_influential_nodes(G):\n",
        "    \"\"\"Compute centrality metrics to identify influential nodes.\"\"\"\n",
        "    print(\"Computing degree centrality...\")\n",
        "    degree_centrality = nx.degree_centrality(G)\n",
        "\n",
        "    print(\"Computing PageRank...\")\n",
        "    pagerank = nx.pagerank(G, weight='weight', max_iter=50)\n",
        "\n",
        "    centrality_df = pd.DataFrame({\n",
        "        'ASIN': list(degree_centrality.keys()),\n",
        "        'DegreeCentrality': list(degree_centrality.values()),\n",
        "        'PageRank': [pagerank.get(node, 0) for node in degree_centrality]\n",
        "    })\n",
        "    influential_nodes = centrality_df.sort_values(by='PageRank', ascending=False).head(10)\n",
        "\n",
        "    return centrality_df, influential_nodes"
      ],
      "metadata": {
        "id": "uDzy8rDAQyxR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_communities(G):\n",
        "    \"\"\"Detect communities using either Leiden (if available), Louvain, or NetworkX's greedy modularity.\"\"\"\n",
        "    if USE_LOUVAIN:\n",
        "        try:\n",
        "            g = ig.Graph.from_networkx(G)\n",
        "            try:\n",
        "                partition = g.community_leiden(objective_function='modularity', weights='weight')\n",
        "                partition_dict = {g.vs[i]['_nx_name']: comm for i, comm in enumerate(partition.membership)}\n",
        "                for node, comm in partition_dict.items():\n",
        "                    G.nodes[node]['community'] = comm\n",
        "                num_communities = len(set(partition_dict.values()))\n",
        "                print(f\"Detected {num_communities} communities using Leiden method\")\n",
        "            except:\n",
        "                partition = louvain_best_partition(G)\n",
        "                for node, comm in partition.items():\n",
        "                    G.nodes[node]['community'] = comm\n",
        "                num_communities = len(set(partition.values()))\n",
        "                print(f\"Detected {num_communities} communities using Louvain method\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in Louvain/Leiden: {e}, falling back to NetworkX\")\n",
        "            communities = list(nx.algorithms.community.greedy_modularity_communities(G))\n",
        "            partition = {}\n",
        "            for comm_id, comm_nodes in enumerate(communities):\n",
        "                for node in comm_nodes:\n",
        "                    partition[node] = comm_id\n",
        "                    G.nodes[node]['community'] = comm_id\n",
        "            num_communities = len(communities)\n",
        "            print(f\"Detected {num_communities} communities using NetworkX greedy modularity\")\n",
        "    else:\n",
        "        communities = list(nx.algorithms.community.greedy_modularity_communities(G))\n",
        "        partition = {}\n",
        "        for comm_id, comm_nodes in enumerate(communities):\n",
        "            for node in comm_nodes:\n",
        "                partition[node] = comm_id\n",
        "                G.nodes[node]['community'] = comm_id\n",
        "        num_communities = len(communities)\n",
        "        print(f\"Detected {num_communities} communities using NetworkX greedy modularity\")\n",
        "    return partition"
      ],
      "metadata": {
        "id": "zmariHfPiBo9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_by_category(G, products_df):\n",
        "    \"\"\"Analyze edges within and between product groups.\"\"\"\n",
        "    asin_to_group = dict(zip(products_df['ASIN'], products_df['Group']))\n",
        "    group_edges = {'within': {}, 'between': {}}\n",
        "    for u, v in G.edges():\n",
        "        group_u = asin_to_group.get(u, 'Unknown')\n",
        "        group_v = asin_to_group.get(v, 'Unknown')\n",
        "        if group_u == group_v:\n",
        "            group_edges['within'][group_u] = group_edges['within'].get(group_u, 0) + 1\n",
        "        else:\n",
        "            edge = tuple(sorted([group_u, group_v]))\n",
        "            group_edges['between'][edge] = group_edges['between'].get(edge, 0) + 1\n",
        "    print(\"Edges within groups:\", group_edges['within'])\n",
        "    print(\"Edges between groups:\", group_edges['between'])"
      ],
      "metadata": {
        "id": "r-pnKPJKiEGe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment_centrality(centrality_df, products_df):\n",
        "    \"\"\"Analyze correlation between centrality and sentiment/reviews.\"\"\"\n",
        "    merged_df = centrality_df.merge(products_df[['ASIN', 'AvgSentiment', 'NumReviews']], on='ASIN')\n",
        "    corr_pagerank_sentiment = merged_df['PageRank'].corr(merged_df['AvgSentiment'])\n",
        "    corr_pagerank_reviews = merged_df['PageRank'].corr(merged_df['NumReviews'])\n",
        "    print(f\"Correlation between PageRank and AvgSentiment: {corr_pagerank_sentiment:.3f}\")\n",
        "    print(f\"Correlation between PageRank and NumReviews: {corr_pagerank_reviews:.3f}\")"
      ],
      "metadata": {
        "id": "9witFPKXiGTc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_graph(G, products_df):\n",
        "    \"\"\"Visualize a subgraph with enhanced features for better analysis.\"\"\"\n",
        "    centrality_df = pd.DataFrame.from_dict(nx.pagerank(G, weight='weight'), orient='index', columns=['PageRank'])\n",
        "\n",
        "    community_sizes = Counter(nx.get_node_attributes(G, 'community').values())\n",
        "    largest_community = max(community_sizes, key=community_sizes.get)\n",
        "    print(f\"Largest community: Community {largest_community} with {community_sizes[largest_community]} nodes\")\n",
        "\n",
        "    nodes_in_largest_community = [node for node, comm in nx.get_node_attributes(G, 'community').items()\n",
        "                                 if comm == largest_community]\n",
        "\n",
        "    community_df = centrality_df.loc[nodes_in_largest_community]\n",
        "    top_nodes = community_df.nlargest(20, 'PageRank').index.tolist()\n",
        "    subgraph = G.subgraph(top_nodes)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    pos = nx.kamada_kawai_layout(subgraph, scale=2)\n",
        "\n",
        "    groups = [subgraph.nodes[node].get('Group', 'Unknown') for node in subgraph.nodes()]\n",
        "    unique_groups = list(set(groups))\n",
        "    colors = plt.cm.Set2(np.linspace(0, 1, len(unique_groups)))\n",
        "    group_to_color = dict(zip(unique_groups, colors))\n",
        "    node_colors = [group_to_color[group] for group in groups]\n",
        "\n",
        "    pagerank_values = [centrality_df.loc[node, 'PageRank'] for node in subgraph.nodes()]\n",
        "    node_sizes = [v * 50000 for v in pagerank_values]\n",
        "\n",
        "    edge_weights = [subgraph[u][v]['weight'] for u, v in subgraph.edges()]\n",
        "    max_weight = max(edge_weights) if edge_weights else 1\n",
        "    edge_widths = [w / max_weight * 5 for w in edge_weights]\n",
        "\n",
        "    nx.draw(subgraph, pos, node_size=node_sizes, node_color=node_colors,\n",
        "            edge_color='gray', width=edge_widths, alpha=0.6)\n",
        "\n",
        "    top_5_nodes = community_df.nlargest(5, 'PageRank').index.tolist()\n",
        "    labels = {}\n",
        "    for node in top_5_nodes:\n",
        "        title = subgraph.nodes[node].get('Title', 'Unknown')[:20]\n",
        "        group = subgraph.nodes[node].get('Group', 'Unknown')\n",
        "        labels[node] = f\"{title}\\n({group})\"\n",
        "    nx.draw_networkx_labels(subgraph, pos, labels, font_size=10, font_color='black', font_weight='bold')\n",
        "\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=group,\n",
        "                              markerfacecolor=group_to_color[group], markersize=10)\n",
        "                       for group in unique_groups]\n",
        "    plt.legend(handles=legend_elements, title=\"Product Groups\", loc='best')\n",
        "\n",
        "    plt.title(f'Co-Purchasing Network (Top Nodes in Community {largest_community})')\n",
        "    plt.savefig(graph_plot)\n",
        "    plt.close()\n",
        "    print(f\"Graph visualization saved to {graph_plot}\")\n",
        "\n",
        "    summary_data = []\n",
        "    for node in top_nodes:\n",
        "        summary_data.append({\n",
        "            'ASIN': node,\n",
        "            'Title': subgraph.nodes[node].get('Title', 'Unknown'),\n",
        "            'Group': subgraph.nodes[node].get('Group', 'Unknown'),\n",
        "            'PageRank': centrality_df.loc[node, 'PageRank'],\n",
        "            'AvgRating': subgraph.nodes[node].get('AvgRating', 0.0),\n",
        "            'NumReviews': subgraph.nodes[node].get('NumReviews', 0),\n",
        "            'Community': subgraph.nodes[node].get('community', -1)\n",
        "        })\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    summary_df.to_csv('visualized_nodes_summary.csv', index=False)\n",
        "    print(\"Summary of visualized nodes saved to visualized_nodes_summary.csv\")\n",
        "    print(summary_df)"
      ],
      "metadata": {
        "id": "KITW_oe0Q1Mr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_graph_interactive(G, products_df):\n",
        "    \"\"\"Visualize an interactive subgraph using Plotly.\"\"\"\n",
        "    centrality_df = pd.DataFrame.from_dict(nx.pagerank(G, weight='weight'), orient='index', columns=['PageRank'])\n",
        "    community_sizes = Counter(nx.get_node_attributes(G, 'community').values())\n",
        "    largest_community = max(community_sizes, key=community_sizes.get)\n",
        "    nodes_in_largest_community = [node for node, comm in nx.get_node_attributes(G, 'community').items()\n",
        "                                 if comm == largest_community]\n",
        "    community_df = centrality_df.loc[nodes_in_largest_community]\n",
        "    top_nodes = community_df.nlargest(20, 'PageRank').index.tolist()\n",
        "    subgraph = G.subgraph(top_nodes)\n",
        "\n",
        "    pos = nx.kamada_kawai_layout(subgraph, scale=2)\n",
        "\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    for edge in subgraph.edges():\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "\n",
        "    edge_trace = go.Scatter(\n",
        "        x=edge_x, y=edge_y,\n",
        "        line=dict(width=0.5, color='#888'),\n",
        "        hoverinfo='none',\n",
        "        mode='lines')\n",
        "\n",
        "    node_x = [pos[node][0] for node in subgraph.nodes()]\n",
        "    node_y = [pos[node][1] for node in subgraph.nodes()]\n",
        "\n",
        "    groups = [subgraph.nodes[node].get('Group', 'Unknown') for node in subgraph.nodes()]\n",
        "    unique_groups = list(set(groups))\n",
        "    colors = plt.cm.Set2(np.linspace(0, 1, len(unique_groups)))\n",
        "    group_to_color = dict(zip(unique_groups, [f'rgb({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)})' for c in colors]))\n",
        "    node_colors = [group_to_color[group] for group in groups]\n",
        "\n",
        "    pagerank_values = [centrality_df.loc[node, 'PageRank'] for node in subgraph.nodes()]\n",
        "    node_sizes = [v * 50000 for v in pagerank_values]\n",
        "\n",
        "    node_text = []\n",
        "    for node in subgraph.nodes():\n",
        "        title = subgraph.nodes[node].get('Title', 'Unknown')\n",
        "        group = subgraph.nodes[node].get('Group', 'Unknown')\n",
        "        pagerank = centrality_df.loc[node, 'PageRank']\n",
        "        avg_rating = subgraph.nodes[node].get('AvgRating', 0.0)\n",
        "        num_reviews = subgraph.nodes[node].get('NumReviews', 0)\n",
        "        community = subgraph.nodes[node].get('community', -1)\n",
        "        node_text.append(f\"ASIN: {node}<br>Title: {title}<br>Group: {group}<br>PageRank: {pagerank:.6f}<br>AvgRating: {avg_rating:.1f}<br>NumReviews: {num_reviews}<br>Community: {community}\")\n",
        "\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y,\n",
        "        mode='markers',\n",
        "        hoverinfo='text',\n",
        "        text=node_text,\n",
        "        marker=dict(\n",
        "            showscale=False,\n",
        "            color=node_colors,\n",
        "            size=node_sizes,\n",
        "            line_width=2))\n",
        "\n",
        "    fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title=f'Co-Purchasing Network (Top Nodes in Community {largest_community})',\n",
        "                        showlegend=False,\n",
        "                        hovermode='closest',\n",
        "                        margin=dict(b=20, l=5, r=5, t=40),\n",
        "                        xaxis=dict(showgrid=False, zeroline=False),\n",
        "                        yaxis=dict(showgrid=False, zeroline=False)))\n",
        "\n",
        "    fig.write_html('copurchase_graph_interactive.html')\n",
        "    print(\"Interactive graph saved to copurchase_graph_interactive.html\")"
      ],
      "metadata": {
        "id": "SL2eGGFfQYJF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_graph_analysis(interactive=False):\n",
        "    \"\"\"Execute graph construction and analysis using preprocessed data.\"\"\"\n",
        "    if not os.path.exists(edges_file):\n",
        "        raise FileNotFoundError(f\"Edges file {edges_file} not found. Preprocessing must complete first.\")\n",
        "    edges_df = pd.read_csv(edges_file)\n",
        "\n",
        "    products_df = None\n",
        "    if os.path.exists(products_file):\n",
        "        products_df = pd.read_csv(products_file)\n",
        "        print(f\"Loaded products data with {len(products_df)} records\")\n",
        "\n",
        "    print(\"Building co-purchasing graph...\")\n",
        "    start = time.time()\n",
        "    G = build_copurchase_graph(edges_df, products_df)\n",
        "    print(f\"Graph construction took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    if products_df is not None:\n",
        "        print(\"Adding node attributes...\")\n",
        "        start = time.time()\n",
        "        add_node_attributes(G, products_df)\n",
        "        print(f\"Adding attributes took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Analyzing graph structure...\")\n",
        "    start = time.time()\n",
        "    metrics, degrees = analyze_graph_structure(G)\n",
        "    print(f\"Structure analysis took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Plotting degree distribution...\")\n",
        "    start = time.time()\n",
        "    plot_degree_distribution(degrees)\n",
        "    print(f\"Degree plotting took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Identifying influential nodes...\")\n",
        "    start = time.time()\n",
        "    centrality_df, influential_nodes = find_influential_nodes(G)\n",
        "    print(f\"Influential nodes computation took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Detecting communities...\")\n",
        "    start = time.time()\n",
        "    partition = detect_communities(G)\n",
        "    print(f\"Community detection took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Analyzing by category...\")\n",
        "    start = time.time()\n",
        "    analyze_by_category(G, products_df)\n",
        "    print(f\"Category analysis took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Analyzing sentiment and reviews...\")\n",
        "    start = time.time()\n",
        "    analyze_sentiment_centrality(centrality_df, products_df)\n",
        "    print(f\"Sentiment analysis took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    metrics_df = pd.DataFrame([metrics])\n",
        "    metrics_df.to_csv(graph_metrics_file, index=False)\n",
        "    print(f\"Graph metrics saved to {graph_metrics_file}\")\n",
        "\n",
        "    centrality_df.to_csv(influential_nodes_file, index=False)\n",
        "    print(f\"Influential nodes saved to {influential_nodes_file}\")\n",
        "    print(\"\\nTop 10 Influential Nodes (by PageRank):\")\n",
        "    print(influential_nodes)\n",
        "\n",
        "    print(\"Visualizing graph...\")\n",
        "    start = time.time()\n",
        "    if interactive:\n",
        "        visualize_graph_interactive(G, products_df)\n",
        "    else:\n",
        "        visualize_graph(G, products_df)\n",
        "    print(f\"Graph visualization took {time.time() - start:.2f} seconds\")"
      ],
      "metadata": {
        "id": "huUsYWHuQ6cx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Part 1: Preprocessing Amazon Metadata\")\n",
        "    start = time.time()\n",
        "    products_enriched, categories_expanded, reviews_processed = preprocess_with_cache(force_reprocess=True)\n",
        "\n",
        "    print(\"\\nPart 2: Graph Construction and Analysis\")\n",
        "    start = time.time()\n",
        "    perform_graph_analysis(interactive=True)\n",
        "    print(f\"Graph analysis took {time.time() - start:.2f} seconds\")\n",
        "    print(\"Graph analysis complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdbdCFv-Q87C",
        "outputId": "46b2f423-2bce-4734-e8cf-99b9c4b3c82f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 1: Preprocessing Amazon Metadata\n",
            "Cached files not found or reprocessing forced. Running preprocessing...\n",
            "Parsing Amazon metadata...\n",
            "Skipped 5870 product rows due to missing required columns or invalid Group/Title.\n",
            "Skipped 0 category rows due to missing or empty category paths.\n",
            "Skipped 0 review rows due to missing required columns.\n",
            "Skipped 557325 edge rows due to missing/empty ASINs or invalid products.\n",
            "Parsing took 127.20 seconds\n",
            "Converting and cleaning data...\n",
            "Filtered out 0 product rows with invalid Group or Title in convert_data.\n",
            "Converting took 184.15 seconds\n",
            "Done. Cleaned data saved to products_enriched.csv, categories_expanded.csv, reviews_processed.csv, and edges.csv\n",
            "Total preprocessing took 311.36 seconds\n",
            "\n",
            "Part 2: Graph Construction and Analysis\n",
            "Loaded products data with 542682 records\n",
            "Building co-purchasing graph...\n",
            "Graph constructed with 366987 nodes and 987903 edges\n",
            "Graph construction took 69.76 seconds\n",
            "Adding node attributes...\n",
            "Adding attributes took 27.78 seconds\n",
            "Analyzing graph structure...\n",
            "Graph Metrics:\n",
            "num_nodes: 366987\n",
            "num_edges: 987903\n",
            "avg_clustering: 0.6944333333333332\n",
            "num_components: 6009\n",
            "avg_degree: 5.383858283808419\n",
            "Structure analysis took 2.53 seconds\n",
            "Plotting degree distribution...\n",
            "Degree distribution plot saved to degree_distribution.png\n",
            "Degree plotting took 0.71 seconds\n",
            "Identifying influential nodes...\n",
            "Computing degree centrality...\n",
            "Computing PageRank...\n",
            "Influential nodes computation took 8.52 seconds\n",
            "Detecting communities...\n",
            "Detected 6443 communities using Leiden method\n",
            "Community detection took 16.45 seconds\n",
            "Analyzing by category...\n",
            "Edges within groups: {'Book': 741124, 'Music': 147483, 'DVD': 39878, 'Video': 16724, 'Toy': 2}\n",
            "Edges between groups: {('Book', 'DVD'): 3394, ('Book', 'Video'): 2261, ('DVD', 'Video'): 31662, ('DVD', 'Music'): 2760, ('Music', 'Video'): 918, ('Book', 'Music'): 1676, ('Music', 'Toy'): 12, ('Sports', 'Video'): 1, ('Book', 'Software'): 3, ('Music', 'Software'): 1, ('Book', 'Video Games'): 3, ('Book', 'Toy'): 1}\n",
            "Category analysis took 3.48 seconds\n",
            "Analyzing sentiment and reviews...\n",
            "Correlation between PageRank and AvgSentiment: 0.094\n",
            "Correlation between PageRank and NumReviews: 0.074\n",
            "Sentiment analysis took 0.52 seconds\n",
            "Graph metrics saved to graph_metrics.csv\n",
            "Influential nodes saved to influential_nodes.csv\n",
            "\n",
            "Top 10 Influential Nodes (by PageRank):\n",
            "             ASIN  DegreeCentrality  PageRank\n",
            "733    B00008LDNZ          0.001496  0.000195\n",
            "3706   0890420254          0.000883  0.000090\n",
            "4923   1557987912          0.000700  0.000081\n",
            "2853   0803606540          0.000488  0.000066\n",
            "3560   0875163238          0.000597  0.000065\n",
            "16679  096290497X          0.000534  0.000057\n",
            "22527  0486291138          0.000621  0.000053\n",
            "24177  0486280861          0.000578  0.000052\n",
            "31982  0130336297          0.000411  0.000052\n",
            "597    0192833723          0.000534  0.000049\n",
            "Visualizing graph...\n",
            "Interactive graph saved to copurchase_graph_interactive.html\n",
            "Graph visualization took 9.98 seconds\n",
            "Graph analysis took 144.20 seconds\n",
            "Graph analysis complete.\n"
          ]
        }
      ]
    }
  ]
}