{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FredSadeghi/Amazon_CoPurchase_Network_Analysis/blob/main/BigDataAmazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K1oj8EP5ZgpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0993eb6-38b2-4bbf-bcc7-a36effb2e5fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.11/dist-packages (0.16)\n",
            "Requirement already satisfied: leidenalg in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: python-igraph in /usr/local/lib/python3.11/dist-packages (0.11.8)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from python-louvain) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from python-louvain) (2.0.2)\n",
            "Requirement already satisfied: igraph<0.12,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from leidenalg) (0.11.8)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from igraph<0.12,>=0.10.0->leidenalg) (1.7.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Successfully imported python-louvain for community detection.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install python-louvain leidenalg python-igraph\n",
        "!pip install networkx matplotlib\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-geometric\n",
        "\n",
        "import gzip\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import igraph as ig\n",
        "import plotly.graph_objects as go\n",
        "import logging\n",
        "\n",
        "\n",
        "# Try importing the Louvain method\n",
        "try:\n",
        "    from community import best_partition as louvain_best_partition\n",
        "    USE_LOUVAIN = True\n",
        "    print(\"Successfully imported python-louvain for community detection.\")\n",
        "except ImportError as e:\n",
        "    print(f\"Failed to import python-louvain: {e}\")\n",
        "    print(\"Falling back to NetworkX's greedy_modularity_communities for community detection.\")\n",
        "    USE_LOUVAIN = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O amazon-meta.txt.gz https://github.com/FredSadeghi/Amazon_CoPurchase_Network_Analysis/releases/download/Data/amazon-meta.txt.gz"
      ],
      "metadata": {
        "id": "GTWJq7XGhqhq",
        "outputId": "7bcac487-dbbd-41af-b1d9-794c6aa12367",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-13 21:26:39--  https://github.com/FredSadeghi/Amazon_CoPurchase_Network_Analysis/releases/download/Data/amazon-meta.txt.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/964118600/ce866930-b0ed-429b-bfeb-185559bef897?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250413%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250413T212639Z&X-Amz-Expires=300&X-Amz-Signature=1d439b25ae3cf87b0539846c56f6c158b27b1e0cd4d7b7447779fb453c956a41&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Damazon-meta.txt.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-04-13 21:26:39--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/964118600/ce866930-b0ed-429b-bfeb-185559bef897?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250413%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250413T212639Z&X-Amz-Expires=300&X-Amz-Signature=1d439b25ae3cf87b0539846c56f6c158b27b1e0cd4d7b7447779fb453c956a41&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Damazon-meta.txt.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 210807517 (201M) [application/octet-stream]\n",
            "Saving to: ‘amazon-meta.txt.gz’\n",
            "\n",
            "amazon-meta.txt.gz  100%[===================>] 201.04M  99.6MB/s    in 2.0s    \n",
            "\n",
            "2025-04-13 21:26:42 (99.6 MB/s) - ‘amazon-meta.txt.gz’ saved [210807517/210807517]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Preprocessiong with Caching"
      ],
      "metadata": {
        "id": "EEScr85QDLgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 1: Preprocessing with Caching ---\n",
        "# Input and output file paths\n",
        "input_file = '/content/amazon-meta.txt.gz'\n",
        "product_output = 'products_cleaned.csv'\n",
        "category_output = 'categories_cleaned.csv'\n",
        "review_output = 'reviews_cleaned.csv'\n",
        "edge_output = 'edges.csv'\n",
        "products_enriched_file = 'products_enriched.csv'\n",
        "categories_expanded_file = 'categories_expanded.csv'\n",
        "reviews_processed_file = 'reviews_processed.csv'"
      ],
      "metadata": {
        "id": "s1GXTDjVZxOw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_amazon_data():\n",
        "    \"\"\"Parse raw Amazon metadata into separate CSV files for products, categories, reviews, and weighted edges.\n",
        "    Exclude products with missing, empty, or 'Unknown' Group values.\n",
        "    Only include edges between valid products.\"\"\"\n",
        "    with gzip.open(input_file, 'rt', encoding='latin-1') as f, \\\n",
        "         open(product_output, 'w', newline='', encoding='utf-8') as prod_out, \\\n",
        "         open(category_output, 'w', newline='', encoding='utf-8') as cat_out, \\\n",
        "         open(review_output, 'w', newline='', encoding='utf-8') as rev_out, \\\n",
        "         open(edge_output, 'w', newline='', encoding='utf-8') as edge_out:\n",
        "\n",
        "        product_writer = csv.writer(prod_out)\n",
        "        category_writer = csv.writer(cat_out)\n",
        "        review_writer = csv.writer(rev_out)\n",
        "        edge_writer = csv.writer(edge_out)\n",
        "\n",
        "        # Write headers\n",
        "        product_writer.writerow(['Id', 'ASIN', 'Title', 'Group', 'SalesRank'])\n",
        "        category_writer.writerow(['ASIN', 'CategoryPath'])\n",
        "        review_writer.writerow(['ASIN', 'CustomerID', 'Rating', 'Votes', 'Helpful', 'Sentiment', 'Date'])\n",
        "        edge_writer.writerow(['SourceASIN', 'TargetASIN', 'Weight'])\n",
        "\n",
        "        current = {}\n",
        "        edge_counts = {}  # To track weights for edges\n",
        "        valid_asins = set()  # Track ASINs of products with valid Group and Title\n",
        "        skipped_products = 0\n",
        "        skipped_reviews = 0\n",
        "        skipped_categories = 0\n",
        "        skipped_edges = 0\n",
        "        total_products = 0\n",
        "\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            # New product entry\n",
        "            if line.startswith(\"Id:\"):\n",
        "                if current.get('ASIN') and current.get('Id'):\n",
        "                    total_products += 1\n",
        "                    # Check if group is present and valid\n",
        "                    group = current.get('group')\n",
        "                    title = current.get('title')\n",
        "                    if (group is None or str(group).strip().lower() == 'unknown' or not str(group).strip() or\n",
        "                        title is None or str(title).strip().lower() == 'unknown' or not str(title).strip()):\n",
        "                        skipped_products += 1\n",
        "                    else:\n",
        "                        product_row = [\n",
        "                            current.get('Id'),\n",
        "                            current.get('ASIN'),\n",
        "                            title,\n",
        "                            group,\n",
        "                            current.get('salesrank', '-1')\n",
        "                        ]\n",
        "                        if None in product_row[:2] or any(str(x).strip() == '' for x in product_row[:2]):\n",
        "                            skipped_products += 1\n",
        "                        else:\n",
        "                            product_writer.writerow(product_row)\n",
        "                            valid_asins.add(current['ASIN'])\n",
        "\n",
        "                            for cat in current.get('categories', []):\n",
        "                                if cat and str(cat).strip():\n",
        "                                    category_writer.writerow([current['ASIN'], cat])\n",
        "                                else:\n",
        "                                    skipped_categories += 1\n",
        "\n",
        "                            for review in current.get('reviews', []):\n",
        "                                sentiment = compute_sentiment(review['rating'])\n",
        "                                review_row = [\n",
        "                                    current['ASIN'],\n",
        "                                    review['customer'],\n",
        "                                    review['rating'],\n",
        "                                    review['votes'],\n",
        "                                    review['helpful'],\n",
        "                                    sentiment,\n",
        "                                    review['date']\n",
        "                                ]\n",
        "                                if None in review_row or any(str(x).strip() == '' for x in review_row[:5]):\n",
        "                                    skipped_reviews += 1\n",
        "                                else:\n",
        "                                    review_writer.writerow(review_row)\n",
        "\n",
        "                            for similar_asin in current.get('similar', []):\n",
        "                                if similar_asin and str(similar_asin).strip():\n",
        "                                    edge = tuple(sorted([current['ASIN'], similar_asin]))\n",
        "                                    edge_counts[edge] = edge_counts.get(edge, 0) + 1\n",
        "                                else:\n",
        "                                    skipped_edges += 1\n",
        "\n",
        "                current = {'categories': [], 'reviews': [], 'similar': []}\n",
        "                current['Id'] = line.split('Id:')[1].strip()\n",
        "\n",
        "            elif line.startswith(\"ASIN:\"):\n",
        "                current['ASIN'] = line.split(\"ASIN:\")[1].strip()\n",
        "\n",
        "            elif 'title:' in line:\n",
        "                match = re.search(r'title:\\s*(.*)', line)\n",
        "                if match:\n",
        "                    current['title'] = match.group(1).strip()\n",
        "\n",
        "            elif 'group:' in line:\n",
        "                match = re.search(r'group:\\s*(.*)', line)\n",
        "                if match:\n",
        "                    current['group'] = match.group(1).strip()\n",
        "\n",
        "            elif 'salesrank:' in line:\n",
        "                match = re.search(r'salesrank:\\s*(.*)', line)\n",
        "                if match:\n",
        "                    current['salesrank'] = match.group(1).strip()\n",
        "\n",
        "            elif line.startswith(\"similar:\"):\n",
        "                parts = line.split()\n",
        "                current['similar'] = parts[2:] if len(parts) > 2 else []\n",
        "\n",
        "            elif line.startswith(\"|\"):\n",
        "                current['categories'].append(line.strip())\n",
        "\n",
        "            elif re.match(r'\\d{4}-\\d{1,2}-\\d{1,2}', line):\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 7:\n",
        "                    review = {\n",
        "                        'date': parts[0],\n",
        "                        'customer': parts[2],\n",
        "                        'rating': int(parts[4]),\n",
        "                        'votes': int(parts[6]),\n",
        "                        'helpful': int(parts[8])\n",
        "                    }\n",
        "                    current['reviews'].append(review)\n",
        "\n",
        "        # Write the last product\n",
        "        if current.get('ASIN') and current.get('Id'):\n",
        "            total_products += 1\n",
        "            group = current.get('group')\n",
        "            title = current.get('title')\n",
        "            if (group is None or str(group).strip().lower() == 'unknown' or not str(group).strip() or\n",
        "                title is None or str(title).strip().lower() == 'unknown' or not str(title).strip()):\n",
        "                skipped_products += 1\n",
        "            else:\n",
        "                product_row = [\n",
        "                    current.get('Id'),\n",
        "                    current.get('ASIN'),\n",
        "                    title,\n",
        "                    group,\n",
        "                    current.get('salesrank', '-1')\n",
        "                ]\n",
        "                if None in product_row[:2] or any(str(x).strip() == '' for x in product_row[:2]):\n",
        "                    skipped_products += 1\n",
        "                else:\n",
        "                    product_writer.writerow(product_row)\n",
        "                    valid_asins.add(current['ASIN'])\n",
        "\n",
        "                    for cat in current.get('categories', []):\n",
        "                        if cat and str(cat).strip():\n",
        "                            category_writer.writerow([current['ASIN'], cat])\n",
        "                        else:\n",
        "                            skipped_categories += 1\n",
        "\n",
        "                    for review in current.get('reviews', []):\n",
        "                        sentiment = compute_sentiment(review['rating'])\n",
        "                        review_row = [\n",
        "                            current['ASIN'],\n",
        "                            review['customer'],\n",
        "                            review['rating'],\n",
        "                            review['votes'],\n",
        "                            review['helpful'],\n",
        "                            sentiment,\n",
        "                            review['date']\n",
        "                        ]\n",
        "                        if None in review_row or any(str(x).strip() == '' for x in review_row[:5]):\n",
        "                            skipped_reviews += 1\n",
        "                        else:\n",
        "                            review_writer.writerow(review_row)\n",
        "\n",
        "                    for similar_asin in current.get('similar', []):\n",
        "                        if similar_asin and str(similar_asin).strip():\n",
        "                            edge = tuple(sorted([current['ASIN'], similar_asin]))\n",
        "                            edge_counts[edge] = edge_counts.get(edge, 0) + 1\n",
        "                        else:\n",
        "                            skipped_edges += 1\n",
        "\n",
        "        # Write edges only for valid ASINs\n",
        "        for (source, target), weight in edge_counts.items():\n",
        "            if source in valid_asins and target in valid_asins:\n",
        "                edge_writer.writerow([source, target, weight])\n",
        "            else:\n",
        "                skipped_edges += 1\n",
        "\n",
        "        # Print summary of skipped rows\n",
        "        print(f\"Processed {total_products} products\")\n",
        "        print(f\"Skipped {skipped_products} products due to missing required columns or invalid Group/Title\")\n",
        "        print(f\"Skipped {skipped_categories} category rows due to missing or empty category paths\")\n",
        "        print(f\"Skipped {skipped_reviews} review rows due to missing required columns\")\n",
        "        print(f\"Skipped {skipped_edges} edge rows due to missing/empty ASINs or invalid products\")"
      ],
      "metadata": {
        "id": "4Y5iTTMiZ8-q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sentiment(rating):\n",
        "    \"\"\"Compute a placeholder sentiment score based on rating (no review text available).\"\"\"\n",
        "    if rating <= 2:\n",
        "        return -1.0  # Negative\n",
        "    elif rating == 3:\n",
        "        return 0.0   # Neutral\n",
        "    else:\n",
        "        return 1.0   # Positive"
      ],
      "metadata": {
        "id": "NM22QwiGaC2-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_data():\n",
        "    \"\"\"Convert CSV files into cleaned, structured pandas DataFrames.\"\"\"\n",
        "    products_df = pd.read_csv(product_output)\n",
        "    categories_df = pd.read_csv(category_output)\n",
        "    reviews_df = pd.read_csv(review_output)\n",
        "\n",
        "    # Drop rows with missing values in critical columns\n",
        "    products_df = products_df.dropna(subset=['Id', 'ASIN'])\n",
        "    categories_df = categories_df.dropna(subset=['ASIN', 'CategoryPath'])\n",
        "    reviews_df = reviews_df.dropna(subset=['ASIN', 'CustomerID', 'Rating', 'Votes', 'Helpful'])\n",
        "\n",
        "    # Double-check filtering for Group and Title\n",
        "    initial_count = len(products_df)\n",
        "    products_df = products_df[\n",
        "        (products_df['Group'].notna()) &\n",
        "        (products_df['Group'].str.strip().str.lower() != 'unknown') &\n",
        "        (products_df['Group'].str.strip() != '') &\n",
        "        (products_df['Title'].notna()) &\n",
        "        (products_df['Title'].str.strip().str.lower() != 'unknown') &\n",
        "        (products_df['Title'].str.strip() != '')\n",
        "    ]\n",
        "    print(f\"Filtered out {initial_count - len(products_df)} product rows with invalid Group or Title in convert_data\")\n",
        "\n",
        "    # Clean products DataFrame\n",
        "    products_df['SalesRank'] = pd.to_numeric(products_df['SalesRank'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "    # Parse categories\n",
        "    def parse_category_path(cat_path):\n",
        "        if pd.isna(cat_path):\n",
        "            return []\n",
        "        parts = cat_path.split(\"|\")\n",
        "        return [re.sub(r\"\\[\\d+\\]\", \"\", part).strip() for part in parts if part]\n",
        "\n",
        "    categories_df['CategoryLevels'] = categories_df['CategoryPath'].apply(parse_category_path)\n",
        "    categories_expanded = categories_df.explode('CategoryLevels')\n",
        "\n",
        "    # Aggregate review metrics\n",
        "    review_summary = reviews_df.groupby('ASIN').agg({\n",
        "        'CustomerID': 'count',\n",
        "        'Rating': 'mean',\n",
        "        'Votes': 'sum',\n",
        "        'Helpful': 'sum',\n",
        "        'Sentiment': 'mean'\n",
        "    }).rename(columns={\n",
        "        'CustomerID': 'NumReviews',\n",
        "        'Rating': 'AvgRating',\n",
        "        'Votes': 'TotalVotes',\n",
        "        'Helpful': 'TotalHelpful',\n",
        "        'Sentiment': 'AvgSentiment'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Join with products\n",
        "    products_enriched = products_df.merge(review_summary, on='ASIN', how='left')\n",
        "    products_enriched = products_enriched.fillna({\n",
        "        'NumReviews': 0, 'AvgRating': 0.0, 'TotalVotes': 0, 'TotalHelpful': 0, 'AvgSentiment': 0.0\n",
        "    })\n",
        "\n",
        "    # Save final cleaned data\n",
        "    products_enriched.to_csv(products_enriched_file, index=False)\n",
        "    categories_expanded.to_csv(categories_expanded_file, index=False)\n",
        "    reviews_df.to_csv(reviews_processed_file, index=False)\n",
        "\n",
        "    return products_enriched, categories_expanded, reviews_df"
      ],
      "metadata": {
        "id": "Ikzj-sKiaDhz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_with_cache(force_reprocess=False):\n",
        "    \"\"\"Preprocess the data with caching. If cached files exist, load them; otherwise, preprocess.\"\"\"\n",
        "    required_files = [\n",
        "        product_output, category_output, review_output, edge_output,\n",
        "        products_enriched_file, categories_expanded_file, reviews_processed_file\n",
        "    ]\n",
        "\n",
        "    all_files_exist = all(os.path.exists(f) for f in required_files)\n",
        "\n",
        "    if all_files_exist and not force_reprocess:\n",
        "        print(\"Cached preprocessed files found. Loading from cache...\")\n",
        "        start = time.time()\n",
        "        products_enriched = pd.read_csv(products_enriched_file)\n",
        "        categories_expanded = pd.read_csv(categories_expanded_file)\n",
        "        reviews_processed = pd.read_csv(reviews_processed_file)\n",
        "        print(f\"Loading cached data took {time.time() - start:.2f} seconds\")\n",
        "        return products_enriched, categories_expanded, reviews_processed\n",
        "\n",
        "    print(\"Cached files not found or reprocessing forced. Running preprocessing...\")\n",
        "    start = time.time()\n",
        "    print(\"Parsing Amazon metadata...\")\n",
        "    parse_start = time.time()\n",
        "    parse_amazon_data()\n",
        "    print(f\"Parsing took {time.time() - parse_start:.2f} seconds\")\n",
        "\n",
        "    print(\"Converting and cleaning data...\")\n",
        "    convert_start = time.time()\n",
        "    products_enriched, categories_expanded, reviews_processed = convert_data()\n",
        "    print(f\"Converting took {time.time() - convert_start:.2f} seconds\")\n",
        "    print(\"Done. Cleaned data saved to products_enriched.csv, categories_expanded.csv, reviews_processed.csv, and edges.csv\")\n",
        "    print(f\"Total preprocessing took {time.time() - start:.2f} seconds\")\n",
        "    return products_enriched, categories_expanded, reviews_processed"
      ],
      "metadata": {
        "id": "BLCbfZ7MCozJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Graph Construction & Analysis"
      ],
      "metadata": {
        "id": "BjY2Qx2NDD2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 2: Graph Construction & Analysis ---\n",
        "# Output file paths\n",
        "edges_file = 'edges.csv'\n",
        "products_file = 'products_enriched.csv'\n",
        "graph_metrics_file = 'graph_metrics.csv'\n",
        "influential_nodes_file = 'influential_nodes.csv'\n",
        "degree_dist_plot = 'degree_distribution.png'\n",
        "graph_plot = 'copurchase_graph.png'"
      ],
      "metadata": {
        "id": "EIbMbIHtQXnM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_copurchase_graph(edges_df, products_df=None):\n",
        "    \"\"\"Construct an undirected graph with weighted edges from co-purchasing relationships.\"\"\"\n",
        "    G = nx.Graph()\n",
        "    valid_asins = set(products_df['ASIN']) if products_df is not None else None\n",
        "\n",
        "    edges = []\n",
        "    for _, row in edges_df.iterrows():\n",
        "        source = row['SourceASIN']\n",
        "        target = row['TargetASIN']\n",
        "        if valid_asins is None or (source in valid_asins and target in valid_asins):\n",
        "            edges.append((source, target, {'weight': row['Weight']}))\n",
        "\n",
        "    G.add_edges_from(edges)\n",
        "    print(f\"Graph constructed with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
        "    return G"
      ],
      "metadata": {
        "id": "kocCgdySQjCL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_node_attributes(G, products_df):\n",
        "    \"\"\"Add product attributes to graph nodes efficiently.\"\"\"\n",
        "    if products_df is not None:\n",
        "        # Only include nodes that are in products_df\n",
        "        products_df = products_df[products_df['ASIN'].isin(G.nodes)]\n",
        "        attributes = {}\n",
        "        for _, row in products_df.iterrows():\n",
        "            asin = row['ASIN']\n",
        "            attributes[asin] = {\n",
        "                'Title': row['Title'],\n",
        "                'Group': row['Group'],\n",
        "                'SalesRank': row.get('SalesRank', -1),\n",
        "                'AvgRating': row.get('AvgRating', 0.0),\n",
        "                'NumReviews': row.get('NumReviews', 0),\n",
        "                'AvgSentiment': row.get('AvgSentiment', 0.0)\n",
        "            }\n",
        "        nx.set_node_attributes(G, attributes)\n",
        "\n",
        "        # Remove nodes that don't have attributes\n",
        "        nodes_to_remove = [node for node in G.nodes if node not in attributes]\n",
        "        if nodes_to_remove:\n",
        "            print(f\"Removed {len(nodes_to_remove)} nodes that lack attributes\")\n",
        "            G.remove_nodes_from(nodes_to_remove)"
      ],
      "metadata": {
        "id": "IOhweSq_Qls4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_graph_structure(G):\n",
        "    \"\"\"Analyze the graph's structure and compute basic metrics.\"\"\"\n",
        "    sampled_nodes = list(G.nodes)[:1000]\n",
        "    sampled_graph = G.subgraph(sampled_nodes)\n",
        "\n",
        "    metrics = {\n",
        "        'num_nodes': G.number_of_nodes(),\n",
        "        'num_edges': G.number_of_edges(),\n",
        "        'avg_clustering': nx.average_clustering(sampled_graph),\n",
        "        'num_components': nx.number_connected_components(G)\n",
        "    }\n",
        "    degrees = [d for _, d in G.degree()]\n",
        "    metrics['avg_degree'] = np.mean(degrees) if degrees else 0\n",
        "\n",
        "    print(\"Graph Metrics:\")\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    return metrics, degrees"
      ],
      "metadata": {
        "id": "UguH1UuKQuoY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_degree_distribution(degrees):\n",
        "    \"\"\"Plot the degree distribution with a power-law fit.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    counts = Counter(degrees)\n",
        "    plt.scatter(counts.keys(), counts.values(), color='blue', alpha=0.5, label='Degree')\n",
        "    degrees = np.array(list(counts.keys()))\n",
        "    frequencies = np.array(list(counts.values()))\n",
        "    mask = (degrees > 0) & (frequencies > 0)\n",
        "    log_degrees = np.log10(degrees[mask])\n",
        "    log_freq = np.log10(frequencies[mask])\n",
        "    if len(log_degrees) > 1:\n",
        "        coeffs = np.polyfit(log_degrees, log_freq, 1)\n",
        "        plt.plot(degrees, 10**(coeffs[1] + coeffs[0] * np.log10(degrees)), 'k--',\n",
        "                 label=f'Power-law fit (γ={-coeffs[0]:.2f})')\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Degree')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Degree Distribution of Co-Purchasing Network')\n",
        "    plt.legend()\n",
        "    plt.savefig(degree_dist_plot)\n",
        "    plt.close()\n",
        "    print(f\"Degree distribution plot saved to {degree_dist_plot}\")"
      ],
      "metadata": {
        "id": "-AZ5jTyJQw58"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_influential_nodes(G):\n",
        "    \"\"\"Compute centrality metrics to identify influential nodes and set PageRank as a node attribute.\"\"\"\n",
        "    print(\"Computing degree centrality...\")\n",
        "    degree_centrality = nx.degree_centrality(G)\n",
        "\n",
        "    print(\"Computing PageRank...\")\n",
        "    pagerank = nx.pagerank(G, weight='weight', max_iter=50)\n",
        "\n",
        "    # Set PageRank as a node attribute\n",
        "    nx.set_node_attributes(G, pagerank, 'PageRank')\n",
        "\n",
        "    centrality_df = pd.DataFrame({\n",
        "        'ASIN': list(degree_centrality.keys()),\n",
        "        'DegreeCentrality': list(degree_centrality.values()),\n",
        "        'PageRank': [pagerank.get(node, 0) for node in degree_centrality]\n",
        "    })\n",
        "    influential_nodes = centrality_df.sort_values(by='PageRank', ascending=False).head(10)\n",
        "\n",
        "    return centrality_df, influential_nodes"
      ],
      "metadata": {
        "id": "uDzy8rDAQyxR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_communities(G, seed=42):\n",
        "    \"\"\"Detect communities using either Leiden (if available), Louvain, or NetworkX's greedy modularity.\n",
        "    Analyze the composition of each community by product group and compute homogeneity statistics.\n",
        "\n",
        "    Parameters:\n",
        "    - G: NetworkX graph\n",
        "    - seed: Random seed for deterministic community detection (default: 42)\n",
        "    \"\"\"\n",
        "    # Set up logging to a file\n",
        "    logging.basicConfig(\n",
        "        filename='community_detection.log',\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "    # Initialize partition dictionary\n",
        "    partition = {}\n",
        "\n",
        "    if USE_LOUVAIN:\n",
        "        try:\n",
        "            # Convert NetworkX graph to igraph for Leiden/Louvain\n",
        "            g = ig.Graph.from_networkx(G)\n",
        "            try:\n",
        "                # Try Leiden algorithm with a fixed seed\n",
        "                partition_igraph = g.community_leiden(\n",
        "                    objective_function='modularity',\n",
        "                    weights='weight',\n",
        "                    n_iterations=10,\n",
        "                    seed=seed\n",
        "                )\n",
        "                partition_dict = {g.vs[i]['_nx_name']: comm for i, comm in enumerate(partition_igraph.membership)}\n",
        "                for node, comm in partition_dict.items():\n",
        "                    G.nodes[node]['community'] = comm\n",
        "                num_communities = len(set(partition_dict.values()))\n",
        "                print(f\"Detected {num_communities} communities using Leiden method\")\n",
        "                partition = partition_dict\n",
        "            except Exception as e:\n",
        "                print(f\"Leiden failed: {e}, falling back to Louvain\")\n",
        "                # Fall back to Louvain with a fixed seed\n",
        "                partition = louvain_best_partition(G, random_state=seed)\n",
        "                for node, comm in partition.items():\n",
        "                    G.nodes[node]['community'] = comm\n",
        "                num_communities = len(set(partition.values()))\n",
        "                print(f\"Detected {num_communities} communities using Louvain method\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in Louvain/Leiden: {e}, falling back to NetworkX\")\n",
        "            # Fall back to NetworkX (deterministic by default)\n",
        "            communities = list(nx.algorithms.community.greedy_modularity_communities(G))\n",
        "            for comm_id, comm_nodes in enumerate(communities):\n",
        "                for node in comm_nodes:\n",
        "                    partition[node] = comm_id\n",
        "                    G.nodes[node]['community'] = comm_id\n",
        "            num_communities = len(communities)\n",
        "            print(f\"Detected {num_communities} communities using NetworkX greedy modularity\")\n",
        "    else:\n",
        "        # Use NetworkX's greedy modularity (deterministic by default)\n",
        "        communities = list(nx.algorithms.community.greedy_modularity_communities(G))\n",
        "        for comm_id, comm_nodes in enumerate(communities):\n",
        "            for node in comm_nodes:\n",
        "                partition[node] = comm_id\n",
        "                G.nodes[node]['community'] = comm_id\n",
        "        num_communities = len(communities)\n",
        "        print(f\"Detected {num_communities} communities using NetworkX greedy modularity\")\n",
        "\n",
        "    # Analyze community composition by product group\n",
        "    community_groups = {}\n",
        "    for node, comm in nx.get_node_attributes(G, 'community').items():\n",
        "        group = G.nodes[node].get('Group', 'N/A')\n",
        "        if comm not in community_groups:\n",
        "            community_groups[comm] = Counter()\n",
        "        community_groups[comm][group] += 1\n",
        "\n",
        "    # Log full community composition to file\n",
        "    logging.info(\"Community Composition (Product Groups, All Communities):\")\n",
        "    sorted_communities = sorted(community_groups.items(), key=lambda x: sum(x[1].values()), reverse=True)\n",
        "    for comm, groups in sorted_communities:\n",
        "        total_nodes = sum(groups.values())\n",
        "        logging.info(f\"Community {comm} ({total_nodes} nodes): {dict(groups)}\")\n",
        "\n",
        "    # Print community composition (limited to top 10 communities to reduce console output)\n",
        "    print(\"\\nCommunity Composition (Product Groups, Top 10):\")\n",
        "    homogeneous_count = 0\n",
        "    total_communities = len(community_groups)\n",
        "    for comm, groups in sorted_communities[:10]:\n",
        "        total_nodes = sum(groups.values())\n",
        "        print(f\"Community {comm} ({total_nodes} nodes): {dict(groups)}\")\n",
        "        if len(groups) == 1:\n",
        "            homogeneous_count += 1\n",
        "\n",
        "    # Compute homogeneity for all communities\n",
        "    for comm, groups in sorted_communities:\n",
        "        if len(groups) == 1:\n",
        "            homogeneous_count += 1\n",
        "\n",
        "    print(f\"\\nHomogeneity Analysis:\")\n",
        "    print(f\"Total communities: {total_communities}\")\n",
        "    print(f\"Homogeneous communities (1 product group): {homogeneous_count}\")\n",
        "    print(f\"Heterogeneous communities (multiple product groups): {total_communities - homogeneous_count}\")\n",
        "    print(f\"Percentage of homogeneous communities: {(homogeneous_count / total_communities * 100):.2f}%\")\n",
        "\n",
        "    return partition"
      ],
      "metadata": {
        "id": "zmariHfPiBo9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_by_category(G, products_df):\n",
        "    \"\"\"Analyze edges within and between product groups.\"\"\"\n",
        "    asin_to_group = dict(zip(products_df['ASIN'], products_df['Group']))\n",
        "    group_edges = {'within': {}, 'between': {}}\n",
        "    for u, v in G.edges():\n",
        "        group_u = asin_to_group.get(u)\n",
        "        group_v = asin_to_group.get(v)\n",
        "        if group_u is None or group_v is None:\n",
        "            continue\n",
        "        if group_u == group_v:\n",
        "            group_edges['within'][group_u] = group_edges['within'].get(group_u, 0) + 1\n",
        "        else:\n",
        "            edge = tuple(sorted([group_u, group_v]))\n",
        "            group_edges['between'][edge] = group_edges['between'].get(edge, 0) + 1\n",
        "    print(\"Edges within groups:\", group_edges['within'])\n",
        "    print(\"Edges between groups:\", group_edges['between'])"
      ],
      "metadata": {
        "id": "r-pnKPJKiEGe"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment_centrality(centrality_df, products_df):\n",
        "    \"\"\"Analyze correlation between centrality and sentiment/reviews.\"\"\"\n",
        "    merged_df = centrality_df.merge(products_df[['ASIN', 'AvgSentiment', 'NumReviews']], on='ASIN')\n",
        "    corr_pagerank_sentiment = merged_df['PageRank'].corr(merged_df['AvgSentiment'])\n",
        "    corr_pagerank_reviews = merged_df['PageRank'].corr(merged_df['NumReviews'])\n",
        "    print(f\"Correlation between PageRank and AvgSentiment: {corr_pagerank_sentiment:.3f}\")\n",
        "    print(f\"Correlation between PageRank and NumReviews: {corr_pagerank_reviews:.3f}\")"
      ],
      "metadata": {
        "id": "9witFPKXiGTc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_graph(G, products_df):\n",
        "    \"\"\"Visualize a subgraph with enhanced features for better analysis.\"\"\"\n",
        "    centrality_df = pd.DataFrame.from_dict(nx.pagerank(G, weight='weight'), orient='index', columns=['PageRank'])\n",
        "\n",
        "    community_sizes = Counter(nx.get_node_attributes(G, 'community').values())\n",
        "    largest_community = max(community_sizes, key=community_sizes.get)\n",
        "    print(f\"Largest community: Community {largest_community} with {community_sizes[largest_community]} nodes\")\n",
        "\n",
        "    nodes_in_largest_community = [node for node, comm in nx.get_node_attributes(G, 'community').items()\n",
        "                                 if comm == largest_community]\n",
        "\n",
        "    community_df = centrality_df.loc[nodes_in_largest_community]\n",
        "    top_nodes = community_df.nlargest(20, 'PageRank').index.tolist()\n",
        "    subgraph = G.subgraph(top_nodes)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    pos = nx.kamada_kawai_layout(subgraph, scale=2)\n",
        "\n",
        "    groups = [subgraph.nodes[node].get('Group') for node in subgraph.nodes()]\n",
        "    unique_groups = list(set(g for g in groups if g is not None))\n",
        "    colors = plt.cm.Set2(np.linspace(0, 1, len(unique_groups)))\n",
        "    group_to_color = dict(zip(unique_groups, colors))\n",
        "    node_colors = [group_to_color[group] for group in groups]\n",
        "\n",
        "    pagerank_values = [centrality_df.loc[node, 'PageRank'] for node in subgraph.nodes()]\n",
        "    node_sizes = [v * 50000 for v in pagerank_values]\n",
        "\n",
        "    edge_weights = [subgraph[u][v]['weight'] for u, v in subgraph.edges()]\n",
        "    max_weight = max(edge_weights) if edge_weights else 1\n",
        "    edge_widths = [w / max_weight * 5 for w in edge_weights]\n",
        "\n",
        "    nx.draw(subgraph, pos, node_size=node_sizes, node_color=node_colors,\n",
        "            edge_color='gray', width=edge_widths, alpha=0.6)\n",
        "\n",
        "    top_5_nodes = community_df.nlargest(5, 'PageRank').index.tolist()\n",
        "    labels = {}\n",
        "    for node in top_5_nodes:\n",
        "        title = subgraph.nodes[node].get('Title', 'N/A')[:20]\n",
        "        group = subgraph.nodes[node].get('Group', 'N/A')\n",
        "        labels[node] = f\"{title}\\n({group})\"\n",
        "    nx.draw_networkx_labels(subgraph, pos, labels, font_size=10, font_color='black', font_weight='bold')\n",
        "\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=group,\n",
        "                              markerfacecolor=group_to_color[group], markersize=10)\n",
        "                       for group in unique_groups]\n",
        "    plt.legend(handles=legend_elements, title=\"Product Groups\", loc='best')\n",
        "\n",
        "    plt.title(f'Co-Purchasing Network (Top Nodes in Community {largest_community})')\n",
        "    plt.savefig(graph_plot)\n",
        "    plt.close()\n",
        "    print(f\"Graph visualization saved to {graph_plot}\")\n",
        "\n",
        "    summary_data = []\n",
        "    for node in top_nodes:\n",
        "        summary_data.append({\n",
        "            'ASIN': node,\n",
        "            'Title': subgraph.nodes[node].get('Title', 'N/A'),\n",
        "            'Group': subgraph.nodes[node].get('Group', 'N/A'),\n",
        "            'PageRank': centrality_df.loc[node, 'PageRank'],\n",
        "            'AvgRating': subgraph.nodes[node].get('AvgRating', 0.0),\n",
        "            'NumReviews': subgraph.nodes[node].get('NumReviews', 0),\n",
        "            'Community': subgraph.nodes[node].get('community', -1)\n",
        "        })\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    summary_df.to_csv('visualized_nodes_summary.csv', index=False)\n",
        "    print(\"Summary of visualized nodes saved to visualized_nodes_summary.csv\")"
      ],
      "metadata": {
        "id": "KITW_oe0Q1Mr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_graph_interactive(G, products_df, community_id=None, max_nodes=None, edge_weight_threshold=1.0):\n",
        "    \"\"\"Visualize an interactive subgraph of all nodes in a specified community using Plotly.\n",
        "\n",
        "    Parameters:\n",
        "    - G: NetworkX graph\n",
        "    - products_df: DataFrame with product information\n",
        "    - community_id: ID of the community to visualize (if None, uses the largest community)\n",
        "    - max_nodes: Maximum number of nodes to display (if None, visualizes all nodes)\n",
        "    - edge_weight_threshold: Minimum edge weight to include in the visualization (default: 1.0)\n",
        "    \"\"\"\n",
        "    centrality_df = pd.DataFrame.from_dict(nx.pagerank(G, weight='weight'), orient='index', columns=['PageRank'])\n",
        "\n",
        "    community_sizes = Counter(nx.get_node_attributes(G, 'community').values())\n",
        "    if not community_sizes:\n",
        "        print(\"No communities found in the graph. Cannot visualize.\")\n",
        "        return\n",
        "\n",
        "    if community_id is None:\n",
        "        selected_community = max(community_sizes, key=community_sizes.get)\n",
        "    else:\n",
        "        selected_community = community_id\n",
        "        if selected_community not in community_sizes:\n",
        "            print(f\"Community {selected_community} does not exist. Available communities: {list(community_sizes.keys())}\")\n",
        "            return\n",
        "\n",
        "    print(f\"Visualizing Community {selected_community} with {community_sizes[selected_community]} nodes\")\n",
        "\n",
        "    nodes_in_community = [node for node, comm in nx.get_node_attributes(G, 'community').items()\n",
        "                          if comm == selected_community]\n",
        "    if not nodes_in_community:\n",
        "        print(f\"No nodes found in Community {selected_community}. Cannot visualize.\")\n",
        "        return\n",
        "\n",
        "    # Apply max_nodes limit if specified, otherwise visualize all nodes\n",
        "    if max_nodes is not None and len(nodes_in_community) > max_nodes:\n",
        "        print(f\"Community {selected_community} has {len(nodes_in_community)} nodes, limiting to top {max_nodes} by PageRank\")\n",
        "        community_df = centrality_df.loc[nodes_in_community]\n",
        "        nodes_to_visualize = community_df.nlargest(max_nodes, 'PageRank').index.tolist()\n",
        "    else:\n",
        "        nodes_to_visualize = nodes_in_community\n",
        "\n",
        "    # Create the subgraph\n",
        "    subgraph = G.subgraph(nodes_to_visualize)\n",
        "\n",
        "    # Filter edges by weight to reduce density, with a fallback\n",
        "    original_edge_count = subgraph.number_of_edges()\n",
        "    edges_to_include = [(u, v) for u, v, d in subgraph.edges(data=True) if d['weight'] >= edge_weight_threshold]\n",
        "    if not edges_to_include:\n",
        "        print(f\"No edges with weight >= {edge_weight_threshold}. Lowering threshold to include at least some edges.\")\n",
        "        edge_weight_threshold = 1.0  # Fallback to a lower threshold\n",
        "        edges_to_include = [(u, v) for u, v, d in subgraph.edges(data=True) if d['weight'] >= edge_weight_threshold]\n",
        "\n",
        "    subgraph = subgraph.edge_subgraph(edges_to_include).copy()\n",
        "\n",
        "    # Ensure the subgraph is connected by taking the largest connected component\n",
        "    if subgraph.number_of_nodes() > 0:\n",
        "        components = list(nx.connected_components(subgraph))\n",
        "        largest_component = max(components, key=len)\n",
        "        subgraph = subgraph.subgraph(largest_component).copy()\n",
        "        print(f\"Using largest connected component with {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges\")\n",
        "\n",
        "    # Print group composition of the visualized nodes\n",
        "    group_counts = Counter(subgraph.nodes[node].get('Group', 'N/A') for node in subgraph.nodes())\n",
        "    print(f\"Group composition of visualized nodes: {dict(group_counts)}\")\n",
        "\n",
        "    if subgraph.number_of_nodes() == 0 or subgraph.number_of_edges() == 0:\n",
        "        print(f\"Subgraph for Community {selected_community} is empty after filtering: {subgraph.number_of_nodes()} nodes, {subgraph.number_of_edges()} edges\")\n",
        "        return\n",
        "\n",
        "    print(f\"Visualizing subgraph with {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges\")\n",
        "\n",
        "    # Use a faster layout for large graphs\n",
        "    try:\n",
        "        pos = nx.spring_layout(subgraph, k=1.0, iterations=50, scale=3)  # Increased k for better spacing\n",
        "    except Exception as e:\n",
        "        print(f\"Spring layout failed: {e}. Falling back to a smaller subgraph.\")\n",
        "        community_df = centrality_df.loc[nodes_to_visualize]\n",
        "        nodes_to_visualize = community_df.nlargest(500, 'PageRank').index.tolist()\n",
        "        subgraph = G.subgraph(nodes_to_visualize)\n",
        "        edges_to_include = [(u, v) for u, v, d in subgraph.edges(data=True) if d['weight'] >= edge_weight_threshold]\n",
        "        subgraph = subgraph.edge_subgraph(edges_to_include).copy()\n",
        "        components = list(nx.connected_components(subgraph))\n",
        "        largest_component = max(components, key=len)\n",
        "        subgraph = subgraph.subgraph(largest_component).copy()\n",
        "        pos = nx.spring_layout(subgraph, k=1.0, iterations=50, scale=3)\n",
        "        print(f\"Fallback subgraph with {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges\")\n",
        "\n",
        "    # Prepare edges\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    for edge in subgraph.edges():\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "\n",
        "    edge_trace = go.Scatter(\n",
        "        x=edge_x, y=edge_y,\n",
        "        line=dict(width=1.0, color='#888'),  # Thinner edges for clarity\n",
        "        hoverinfo='none',\n",
        "        mode='lines',\n",
        "        showlegend=False\n",
        "    )\n",
        "\n",
        "    # Prepare nodes\n",
        "    node_x = [pos[node][0] for node in subgraph.nodes()]\n",
        "    node_y = [pos[node][1] for node in subgraph.nodes()]\n",
        "\n",
        "    # Assign colors to groups\n",
        "    groups = [subgraph.nodes[node].get('Group') for node in subgraph.nodes()]\n",
        "    unique_groups = list(set(g for g in groups if g is not None))\n",
        "    if not unique_groups:\n",
        "        print(\"No valid groups found for nodes in the subgraph. Using a default color.\")\n",
        "        node_colors = ['rgb(31, 119, 180)'] * len(subgraph.nodes())\n",
        "        group_to_color = {}\n",
        "    else:\n",
        "        colors = {\n",
        "            'Book': 'rgb(31, 119, 180)',  # Blue\n",
        "            'Music': 'rgb(255, 127, 14)',  # Orange\n",
        "            'DVD': 'rgb(44, 160, 44)',  # Green\n",
        "            'Video': 'rgb(214, 39, 40)',  # Red\n",
        "        }\n",
        "        default_color = 'rgb(127, 127, 127)'  # Gray for others\n",
        "        group_to_color = {group: colors.get(group, default_color) for group in unique_groups}\n",
        "        node_colors = [group_to_color[group] for group in groups]\n",
        "\n",
        "    # Scale node sizes based on PageRank\n",
        "    pagerank_values = [centrality_df.loc[node, 'PageRank'] for node in subgraph.nodes()]\n",
        "    node_sizes = [v * 100000 for v in pagerank_values]\n",
        "    node_sizes = [max(5, min(size, 20)) for size in node_sizes]  # Adjusted max size\n",
        "\n",
        "    # Prepare node hover text\n",
        "    node_text = []\n",
        "    for node in subgraph.nodes():\n",
        "        title = subgraph.nodes[node].get('Title', 'N/A')\n",
        "        group = subgraph.nodes[node].get('Group', 'N/A')\n",
        "        pagerank = centrality_df.loc[node, 'PageRank']\n",
        "        avg_rating = subgraph.nodes[node].get('AvgRating', 0.0)\n",
        "        num_reviews = subgraph.nodes[node].get('NumReviews', 0)\n",
        "        community = subgraph.nodes[node].get('community', -1)\n",
        "        node_text.append(f\"ASIN: {node}<br>Title: {title}<br>Group: {group}<br>PageRank: {pagerank:.6f}<br>AvgRating: {avg_rating:.1f}<br>NumReviews: {num_reviews}<br>Community: {community}\")\n",
        "\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y,\n",
        "        mode='markers',\n",
        "        hoverinfo='text',\n",
        "        text=node_text,\n",
        "        marker=dict(\n",
        "            showscale=False,\n",
        "            color=node_colors,\n",
        "            size=node_sizes,\n",
        "            line=dict(width=1, color='black'),\n",
        "            opacity=0.8\n",
        "        ),\n",
        "        showlegend=False\n",
        "    )\n",
        "\n",
        "    # Create the figure\n",
        "    fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title=f'Co-Purchasing Network (Community {selected_community})',\n",
        "                        showlegend=True,\n",
        "                        hovermode='closest',\n",
        "                        margin=dict(b=20, l=5, r=5, t=40),\n",
        "                        xaxis=dict(showgrid=False, zeroline=False),\n",
        "                        yaxis=dict(showgrid=False, zeroline=False),\n",
        "                        legend=dict(\n",
        "                            title=\"Product Groups\",\n",
        "                            x=1.05,\n",
        "                            y=1,\n",
        "                            xanchor='left',\n",
        "                            yanchor='top'\n",
        "                        )\n",
        "                    ))\n",
        "\n",
        "    # Add legend entries for each group\n",
        "    for group, color in group_to_color.items():\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[None], y=[None],\n",
        "            mode='markers',\n",
        "            name=group,\n",
        "            marker=dict(size=10, color=color, line=dict(width=2, color='black')),\n",
        "            showlegend=True\n",
        "        ))\n",
        "\n",
        "    fig.write_html('copurchase_graph_interactive.html')\n",
        "    print(\"Interactive graph saved to copurchase_graph_interactive.html\")"
      ],
      "metadata": {
        "id": "SL2eGGFfQYJF"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_graph_analysis(interactive=False, community_id=None, max_nodes=None, seed=42, edge_weight_threshold=1.0):\n",
        "    \"\"\"Execute graph construction and analysis using preprocessed data.\"\"\"\n",
        "    if not os.path.exists(edges_file):\n",
        "        raise FileNotFoundError(f\"Edges file {edges_file} not found. Preprocessing must complete first.\")\n",
        "    edges_df = pd.read_csv(edges_file)\n",
        "\n",
        "    products_df = None\n",
        "    if os.path.exists(products_file):\n",
        "        products_df = pd.read_csv(products_file)\n",
        "        print(f\"Loaded products data with {len(products_df)} records\")\n",
        "\n",
        "    print(\"Building co-purchasing graph...\")\n",
        "    start = time.time()\n",
        "    G = build_copurchase_graph(edges_df, products_df)\n",
        "    print(f\"Graph construction took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    if products_df is not None:\n",
        "        print(\"Adding node attributes...\")\n",
        "        start = time.time()\n",
        "        add_node_attributes(G, products_df)\n",
        "        print(f\"Adding attributes took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Analyzing graph structure...\")\n",
        "    start = time.time()\n",
        "    metrics, degrees = analyze_graph_structure(G)\n",
        "    print(f\"Structure analysis took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Plotting degree distribution...\")\n",
        "    start = time.time()\n",
        "    plot_degree_distribution(degrees)\n",
        "    print(f\"Degree plotting took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Identifying influential nodes...\")\n",
        "    start = time.time()\n",
        "    centrality_df, influential_nodes = find_influential_nodes(G)\n",
        "    print(f\"Influential nodes computation took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Detecting communities...\")\n",
        "    start = time.time()\n",
        "    partition = detect_communities(G, seed=seed)\n",
        "    print(f\"Community detection took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Analyzing by category...\")\n",
        "    start = time.time()\n",
        "    analyze_by_category(G, products_df)\n",
        "    print(f\"Category analysis took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Analyzing sentiment and reviews...\")\n",
        "    start = time.time()\n",
        "    analyze_sentiment_centrality(centrality_df, products_df)\n",
        "    print(f\"Sentiment analysis took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    metrics_df = pd.DataFrame([metrics])\n",
        "    metrics_df.to_csv(graph_metrics_file, index=False)\n",
        "    print(f\"Graph metrics saved to {graph_metrics_file}\")\n",
        "\n",
        "    centrality_df.to_csv(influential_nodes_file, index=False)\n",
        "    print(f\"Influential nodes saved to {influential_nodes_file}\")\n",
        "    print(\"\\nTop 10 Influential Nodes (by PageRank):\")\n",
        "    print(influential_nodes)\n",
        "\n",
        "    print(\"Visualizing graph...\")\n",
        "    start = time.time()\n",
        "    try:\n",
        "        if interactive:\n",
        "            visualize_graph_interactive(G, products_df, community_id=community_id, max_nodes=max_nodes, edge_weight_threshold=edge_weight_threshold)\n",
        "        else:\n",
        "            visualize_graph(G, products_df)\n",
        "    except Exception as e:\n",
        "        print(f\"Interactive visualization failed: {e}. Falling back to static visualization.\")\n",
        "        visualize_graph(G, products_df)\n",
        "    print(f\"Graph visualization took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    # Verify node attributes before exporting\n",
        "    print(\"Checking node attributes before exporting to GEXF...\")\n",
        "    sample_node = list(G.nodes)[0]\n",
        "    print(f\"Attributes for sample node {sample_node}: {G.nodes[sample_node]}\")\n",
        "    if 'PageRank' not in G.nodes[sample_node]:\n",
        "        print(\"Warning: PageRank attribute is missing!\")\n",
        "    if 'Group' not in G.nodes[sample_node]:\n",
        "        print(\"Warning: Group attribute is missing!\")\n",
        "\n",
        "    # Export the specified community to GEXF for visualization in Gephi\n",
        "    if community_id is not None:\n",
        "        nodes_in_community = [node for node, comm in nx.get_node_attributes(G, 'community').items() if comm == community_id]\n",
        "        subgraph = G.subgraph(nodes_in_community)\n",
        "        nx.write_gexf(subgraph, f\"community_{community_id}.gexf\")\n",
        "        print(f\"Exported Community {community_id} to community_{community_id}.gexf for visualization in Gephi\")"
      ],
      "metadata": {
        "id": "huUsYWHuQ6cx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 Predictive Modeling - Link Prediction using GNNs"
      ],
      "metadata": {
        "id": "BWiDwjVYDSzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 3 Predictive Modeling - Link Prediction using GNNs\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.nn import GCNConv, GAE\n",
        "from torch_geometric.utils import from_networkx, negative_sampling\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import gc\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "bumag_oj-5RZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 1: Build the co-purchase graph\n",
        "def build_copurchase_graphs(subsample_frac=0.05):\n",
        "    \"\"\"Build a co-purchasing network with subsampling.\"\"\"\n",
        "    start_time = time.time()\n",
        "    edges_df = pd.read_csv('edges.csv')\n",
        "\n",
        "    # Subsample edges\n",
        "    edges_df = edges_df.sample(frac=subsample_frac, random_state=42)\n",
        "    print(f\"Subsampled to {len(edges_df)} edges ({subsample_frac*100:.1f}%)\")\n",
        "\n",
        "    G = nx.Graph()\n",
        "    for _, row in edges_df.iterrows():\n",
        "        G.add_edge(row['SourceASIN'], row['TargetASIN'])\n",
        "\n",
        "    G.remove_nodes_from(list(nx.isolates(G)))\n",
        "    print(f\"Graph constructed with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
        "    print(f\"Graph construction took {time.time() - start_time:.2f} seconds\")\n",
        "    return G"
      ],
      "metadata": {
        "id": "SXXdOjjN_Ril",
        "outputId": "6e12d26d-67c2-4312-e6b1-18450821eed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Prepare graph data for GNN\n",
        "def prepare_graph_data(G):\n",
        "    \"\"\"Convert NetworkX graph to PyTorch Geometric Data with numerical features.\"\"\"\n",
        "    data = from_networkx(G)\n",
        "    node_mapping = {node: i for i, node in enumerate(G.nodes())}\n",
        "    data.node_mapping = node_mapping\n",
        "\n",
        "    # Load product data\n",
        "    products_df = pd.read_csv('products_enriched.csv')\n",
        "\n",
        "    # Numerical features: SalesRank, AvgRating\n",
        "    numerical_features = products_df.set_index('ASIN')[['SalesRank', 'AvgRating']].reindex(G.nodes())\n",
        "    numerical_features = numerical_features.fillna(0)\n",
        "\n",
        "    # Debug: Check dtypes and sample values\n",
        "    print(\"Numerical features dtypes:\")\n",
        "    print(numerical_features.dtypes)\n",
        "    print(\"Sample of numerical features:\")\n",
        "    print(numerical_features.head())\n",
        "\n",
        "    # Ensure numerical features are float\n",
        "    numerical_features = numerical_features.astype(float)\n",
        "\n",
        "    # Normalize numerical features\n",
        "    scaler = StandardScaler()\n",
        "    numerical_features = scaler.fit_transform(numerical_features)\n",
        "\n",
        "    # Categorical feature: Group (one-hot encoded)\n",
        "    group_encoded = pd.get_dummies(products_df.set_index('ASIN')['Group']).reindex(G.nodes())\n",
        "    group_encoded = group_encoded.fillna(0)\n",
        "\n",
        "    # Debug: Check dtypes and sample values\n",
        "    print(\"Group encoded dtypes:\")\n",
        "    print(group_encoded.dtypes)\n",
        "    print(\"Sample of group encoded:\")\n",
        "    print(group_encoded.head())\n",
        "\n",
        "    # Ensure group_encoded is numerical\n",
        "    group_encoded = group_encoded.astype(float)\n",
        "\n",
        "    # Combine features\n",
        "    node_features = np.hstack([numerical_features, group_encoded.values])\n",
        "\n",
        "    # Debug: Check final node features dtype\n",
        "    print(\"Node features dtype after hstack:\", node_features.dtype)\n",
        "    print(\"Sample of node features:\")\n",
        "    print(node_features[:5])\n",
        "\n",
        "    data.x = torch.tensor(node_features, dtype=torch.float).to(device)\n",
        "\n",
        "    transform = RandomLinkSplit(\n",
        "        num_val=0.1,\n",
        "        num_test=0.1,\n",
        "        is_undirected=True,\n",
        "        add_negative_train_samples=False\n",
        "    )\n",
        "    train_data, val_data, test_data = transform(data)\n",
        "\n",
        "    gc.collect()\n",
        "    return train_data.to(device), val_data.to(device), test_data.to(device), node_mapping"
      ],
      "metadata": {
        "id": "Pw74thUa_RbJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define the GNN model (GAE)\n",
        "class GNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GNNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class GAEModel(GAE):\n",
        "    def __init__(self, encoder):\n",
        "        super(GAEModel, self).__init__(encoder=encoder)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        z_src = z[edge_index[0]]\n",
        "        z_dst = z[edge_index[1]]\n",
        "        return F.cosine_similarity(z_src, z_dst, dim=1)"
      ],
      "metadata": {
        "id": "w9gFUqob_RRt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train the GAE model\n",
        "def train_gae(model, train_data, optimizer, num_epochs=200):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        z = model.encode(train_data.x, train_data.edge_index)\n",
        "        loss = model.recon_loss(z, train_data.edge_index)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "OysXLdGx_ROu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Evaluate the model\n",
        "def evaluate_gae(model, data, edge_label_index, edge_label):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(data.x, data.edge_index)\n",
        "        edge_probs = model.decode(z, edge_label_index)\n",
        "        edge_probs = edge_probs.cpu().numpy()\n",
        "        edge_label = edge_label.cpu().numpy()\n",
        "        auc = roc_auc_score(edge_label, edge_probs)\n",
        "        ap = average_precision_score(edge_label, edge_probs)\n",
        "    return auc, ap"
      ],
      "metadata": {
        "id": "XrGSM9tg_RL3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Recommend products using GNN\n",
        "def recommend_products(model, data, node_mapping, asin, top_k=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(data.x, data.edge_index)\n",
        "        print(\"Sample of node embeddings (first 5 nodes, first 3 dimensions):\")\n",
        "        print(z[:5, :3])\n",
        "        print(\"Embedding variance (should be non-zero):\")\n",
        "        print(torch.var(z, dim=0).mean().item())\n",
        "\n",
        "        if asin not in node_mapping:\n",
        "            raise ValueError(f\"ASIN {asin} not found in node mapping\")\n",
        "        node_idx = node_mapping[asin]\n",
        "        print(f\"Input ASIN: {asin}, Node index: {node_idx}\")\n",
        "\n",
        "        z_src = z[node_idx].unsqueeze(0)\n",
        "        z_dst = z\n",
        "        scores = F.cosine_similarity(z_src, z_dst, dim=1)\n",
        "\n",
        "        print(\"Sample of scores (first 10):\")\n",
        "        print(scores[:10])\n",
        "        print(\"Score variance (should be non-zero):\")\n",
        "        print(torch.var(scores).item())\n",
        "\n",
        "        scores[node_idx] = -float('inf')\n",
        "        _, top_indices = scores.topk(top_k)\n",
        "        top_indices = top_indices.cpu().numpy()\n",
        "\n",
        "        print(\"Top-k indices:\", top_indices)\n",
        "\n",
        "        reverse_mapping = {v: k for k, v in node_mapping.items()}\n",
        "        recommended_asins = [reverse_mapping[idx] for idx in top_indices]\n",
        "        return recommended_asins"
      ],
      "metadata": {
        "id": "NLSWP6WO_RHP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run the recommendation system\n",
        "def run_recommendation_system(subsample_frac=0.05, num_epochs=200, top_k=5):\n",
        "    # Build the graph\n",
        "    print(\"Building co-purchase graph...\")\n",
        "    G = build_copurchase_graphs(subsample_frac=subsample_frac)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"Preparing graph data for GNN...\")\n",
        "    train_data, val_data, test_data, node_mapping = prepare_graph_data(G)\n",
        "\n",
        "    # Initialize model\n",
        "    in_channels = train_data.x.shape[1]\n",
        "    hidden_channels = 32\n",
        "    out_channels = 16\n",
        "    encoder = GNNEncoder(in_channels, hidden_channels, out_channels).to(device)\n",
        "    model = GAEModel(encoder).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    # Train\n",
        "    print(\"Training GNN model...\")\n",
        "    model = train_gae(model, train_data, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"Evaluating model...\")\n",
        "    val_neg_edge_index = negative_sampling(\n",
        "        edge_index=val_data.edge_index,\n",
        "        num_nodes=val_data.num_nodes,\n",
        "        num_neg_samples=val_data.edge_label_index.size(1) // 2\n",
        "    )\n",
        "    val_edge_label_index = torch.cat([val_data.edge_label_index, val_neg_edge_index], dim=-1)\n",
        "    val_edge_label = torch.cat([\n",
        "        torch.ones(val_data.edge_label_index.size(1)),\n",
        "        torch.zeros(val_neg_edge_index.size(1))\n",
        "    ], dim=0).to(device)\n",
        "    val_auc, val_ap = evaluate_gae(model, val_data, val_edge_label_index, val_edge_label)\n",
        "    print(f\"Validation AUC: {val_auc:.4f}, AP: {val_ap:.4f}\")\n",
        "\n",
        "    test_neg_edge_index = negative_sampling(\n",
        "        edge_index=test_data.edge_index,\n",
        "        num_nodes=test_data.num_nodes,\n",
        "        num_neg_samples=test_data.edge_label_index.size(1) // 2\n",
        "    )\n",
        "    test_edge_label_index = torch.cat([test_data.edge_label_index, test_neg_edge_index], dim=-1)\n",
        "    test_edge_label = torch.cat([\n",
        "        torch.ones(test_data.edge_label_index.size(1)),\n",
        "        torch.zeros(test_neg_edge_index.size(1))\n",
        "    ], dim=0).to(device)\n",
        "    test_auc, test_ap = evaluate_gae(model, test_data, test_edge_label_index, test_edge_label)\n",
        "    print(f\"Test AUC: {test_auc:.4f}, AP: {test_ap:.4f}\")\n",
        "\n",
        "    # Recommend with GNN and Common Neighbors\n",
        "    products_df = pd.read_csv('products_enriched.csv')\n",
        "    random_asin = random.choice(list(node_mapping.keys()))\n",
        "\n",
        "    # GNN Recommendations\n",
        "    print(f\"\\nRecommending products for randomly selected ASIN (GNN): {random_asin}\")\n",
        "    recommended_asins = recommend_products(model, test_data, node_mapping, random_asin, top_k=top_k)\n",
        "    print(\"Recommended ASINs:\", recommended_asins)\n",
        "    recommended_titles = products_df[products_df['ASIN'].isin(recommended_asins)][['ASIN', 'Title']]\n",
        "    print(\"Recommended Products:\")\n",
        "    print(recommended_titles)"
      ],
      "metadata": {
        "id": "dfy0ueXiWjtE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Execution"
      ],
      "metadata": {
        "id": "QVQTAi3YEpWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Part 1: Preprocessing Amazon Metadata\")\n",
        "    start = time.time()\n",
        "    products_enriched, categories_expanded, reviews_processed = preprocess_with_cache(force_reprocess=False)\n",
        "\n",
        "    print(\"\\nPart 2: Graph Construction and Analysis\")\n",
        "    start = time.time()\n",
        "    perform_graph_analysis(\n",
        "        interactive=True,\n",
        "        community_id=18,  # Visualize Community 18\n",
        "        max_nodes=1000,   # Limit to 1000 nodes\n",
        "        seed=42,\n",
        "        edge_weight_threshold=1.0  # Lowered to ensure edges are included\n",
        "    )\n",
        "    print(f\"Graph analysis took {time.time() - start:.2f} seconds\")\n",
        "    print(\"Graph analysis complete.\")\n",
        "\n",
        "    print(\"\\nPart 3: Predictive Modeling - Link Prediction using GNNs\")\n",
        "    run_recommendation_system(subsample_frac=0.05, num_epochs=200, top_k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdbdCFv-Q87C",
        "outputId": "aec580d3-5707-4bb0-87f1-5e1f87e1f112"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 1: Preprocessing Amazon Metadata\n",
            "Cached preprocessed files found. Loading from cache...\n",
            "Loading cached data took 43.32 seconds\n",
            "\n",
            "Part 2: Graph Construction and Analysis\n",
            "Loaded products data with 542682 records\n",
            "Building co-purchasing graph...\n",
            "Graph constructed with 366987 nodes and 987903 edges\n",
            "Graph construction took 94.96 seconds\n",
            "Adding node attributes...\n",
            "Adding attributes took 33.85 seconds\n",
            "Analyzing graph structure...\n",
            "Graph Metrics:\n",
            "num_nodes: 366987\n",
            "num_edges: 987903\n",
            "avg_clustering: 0.694433333333333\n",
            "num_components: 6009\n",
            "avg_degree: 5.383858283808419\n",
            "Structure analysis took 2.36 seconds\n",
            "Plotting degree distribution...\n",
            "Degree distribution plot saved to degree_distribution.png\n",
            "Degree plotting took 0.69 seconds\n",
            "Identifying influential nodes...\n",
            "Computing degree centrality...\n",
            "Computing PageRank...\n",
            "Influential nodes computation took 15.53 seconds\n",
            "Detecting communities...\n",
            "Leiden failed: unexpected keyword argument, falling back to Louvain\n",
            "Detected 6278 communities using Louvain method\n",
            "\n",
            "Community Composition (Product Groups, Top 10):\n",
            "Community 18 (13101 nodes): {'Book': 12849, 'Music': 88, 'DVD': 42, 'Video': 122}\n",
            "Community 100 (11305 nodes): {'Video': 4159, 'DVD': 4887, 'Music': 1229, 'Book': 1030}\n",
            "Community 0 (10099 nodes): {'Book': 10059, 'Video': 28, 'DVD': 7, 'Music': 5}\n",
            "Community 27 (9826 nodes): {'Book': 9817, 'Video': 9}\n",
            "Community 5 (8725 nodes): {'Book': 8503, 'DVD': 92, 'Video': 110, 'Music': 20}\n",
            "Community 262 (7864 nodes): {'Book': 7787, 'DVD': 35, 'Music': 7, 'Video': 35}\n",
            "Community 11 (6873 nodes): {'Book': 6849, 'Video': 23, 'DVD': 1}\n",
            "Community 57 (6863 nodes): {'Book': 6857, 'DVD': 1, 'Video': 5}\n",
            "Community 55 (6729 nodes): {'Book': 6653, 'DVD': 18, 'Music': 31, 'Video': 27}\n",
            "Community 21 (5995 nodes): {'Book': 5964, 'Video': 23, 'DVD': 6, 'Music': 2}\n",
            "\n",
            "Homogeneity Analysis:\n",
            "Total communities: 6278\n",
            "Homogeneous communities (1 product group): 5905\n",
            "Heterogeneous communities (multiple product groups): 373\n",
            "Percentage of homogeneous communities: 94.06%\n",
            "Community detection took 228.88 seconds\n",
            "Analyzing by category...\n",
            "Edges within groups: {'Book': 741124, 'Music': 147483, 'DVD': 39878, 'Video': 16724, 'Toy': 2}\n",
            "Edges between groups: {('Book', 'DVD'): 3394, ('Book', 'Video'): 2261, ('DVD', 'Video'): 31662, ('DVD', 'Music'): 2760, ('Music', 'Video'): 918, ('Book', 'Music'): 1676, ('Music', 'Toy'): 12, ('Sports', 'Video'): 1, ('Book', 'Software'): 3, ('Music', 'Software'): 1, ('Book', 'Video Games'): 3, ('Book', 'Toy'): 1}\n",
            "Category analysis took 2.79 seconds\n",
            "Analyzing sentiment and reviews...\n",
            "Correlation between PageRank and AvgSentiment: 0.094\n",
            "Correlation between PageRank and NumReviews: 0.074\n",
            "Sentiment analysis took 0.56 seconds\n",
            "Graph metrics saved to graph_metrics.csv\n",
            "Influential nodes saved to influential_nodes.csv\n",
            "\n",
            "Top 10 Influential Nodes (by PageRank):\n",
            "             ASIN  DegreeCentrality  PageRank\n",
            "733    B00008LDNZ          0.001496  0.000195\n",
            "3706   0890420254          0.000883  0.000090\n",
            "4923   1557987912          0.000700  0.000081\n",
            "2853   0803606540          0.000488  0.000066\n",
            "3560   0875163238          0.000597  0.000065\n",
            "16679  096290497X          0.000534  0.000057\n",
            "22527  0486291138          0.000621  0.000053\n",
            "24177  0486280861          0.000578  0.000052\n",
            "31982  0130336297          0.000411  0.000052\n",
            "597    0192833723          0.000534  0.000049\n",
            "Visualizing graph...\n",
            "Visualizing Community 18 with 13101 nodes\n",
            "Community 18 has 13101 nodes, limiting to top 1000 by PageRank\n",
            "Using largest connected component with 518 nodes and 1203 edges\n",
            "Group composition of visualized nodes: {'Book': 506, 'DVD': 4, 'Video': 4, 'Music': 4}\n",
            "Visualizing subgraph with 518 nodes and 1203 edges\n",
            "Interactive graph saved to copurchase_graph_interactive.html\n",
            "Graph visualization took 15.49 seconds\n",
            "Checking node attributes before exporting to GEXF...\n",
            "Attributes for sample node 0804215715: {'Title': 'Witness of Preaching', 'Group': 'Book', 'SalesRank': 93405, 'AvgRating': 4.666666666666667, 'NumReviews': 3.0, 'AvgSentiment': 1.0, 'PageRank': 1.0243873799344187e-05, 'community': 0}\n",
            "Exported Community 18 to community_18.gexf for visualization in Gephi\n",
            "Graph analysis took 403.51 seconds\n",
            "Graph analysis complete.\n",
            "\n",
            "Part 3: Predictive Modeling - Link Prediction using GNNs\n",
            "Building co-purchase graph...\n",
            "Subsampled to 49395 edges (5.0%)\n",
            "Graph constructed with 81400 nodes and 49395 edges\n",
            "Graph construction took 5.18 seconds\n",
            "Preparing graph data for GNN...\n",
            "Numerical features dtypes:\n",
            "SalesRank      int64\n",
            "AvgRating    float64\n",
            "dtype: object\n",
            "Sample of numerical features:\n",
            "            SalesRank  AvgRating\n",
            "ASIN                            \n",
            "0156010941     244813   4.475000\n",
            "0385333382     368161   4.403846\n",
            "0886773768     139836   4.157895\n",
            "0886774764     188475   5.000000\n",
            "B000002N2X      53939   3.923077\n",
            "Group encoded dtypes:\n",
            "Baby Product    bool\n",
            "Book            bool\n",
            "CE              bool\n",
            "DVD             bool\n",
            "Music           bool\n",
            "Software        bool\n",
            "Sports          bool\n",
            "Toy             bool\n",
            "Video           bool\n",
            "Video Games     bool\n",
            "dtype: object\n",
            "Sample of group encoded:\n",
            "            Baby Product   Book     CE    DVD  Music  Software  Sports    Toy  \\\n",
            "ASIN                                                                            \n",
            "0156010941         False   True  False  False  False     False   False  False   \n",
            "0385333382         False   True  False  False  False     False   False  False   \n",
            "0886773768         False   True  False  False  False     False   False  False   \n",
            "0886774764         False   True  False  False  False     False   False  False   \n",
            "B000002N2X         False  False  False  False   True     False   False  False   \n",
            "\n",
            "            Video  Video Games  \n",
            "ASIN                            \n",
            "0156010941  False        False  \n",
            "0385333382  False        False  \n",
            "0886773768  False        False  \n",
            "0886774764  False        False  \n",
            "B000002N2X  False        False  \n",
            "Node features dtype after hstack: float64\n",
            "Sample of node features:\n",
            "[[ 0.0657381   0.42243341  0.          1.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.        ]\n",
            " [ 0.54035474  0.37512867  0.          1.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.        ]\n",
            " [-0.33819088  0.21161442  0.          1.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.        ]\n",
            " [-0.15103844  0.77146568  0.          1.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.        ]\n",
            " [-0.66870411  0.05550205  0.          0.          0.          0.\n",
            "   1.          0.          0.          0.          0.          0.        ]]\n",
            "Training GNN model...\n",
            "Epoch 20/200, Loss: 1.0704\n",
            "Epoch 40/200, Loss: 1.0217\n",
            "Epoch 60/200, Loss: 0.9967\n",
            "Epoch 80/200, Loss: 0.9753\n",
            "Epoch 100/200, Loss: 0.9645\n",
            "Epoch 120/200, Loss: 0.9535\n",
            "Epoch 140/200, Loss: 0.9436\n",
            "Epoch 160/200, Loss: 0.9397\n",
            "Epoch 180/200, Loss: 0.9355\n",
            "Epoch 200/200, Loss: 0.9276\n",
            "Evaluating model...\n",
            "Validation AUC: 0.5922, AP: 0.7537\n",
            "Test AUC: 0.5946, AP: 0.7562\n",
            "\n",
            "Recommending products for randomly selected ASIN (GNN): 3931126358\n",
            "Sample of node embeddings (first 5 nodes, first 3 dimensions):\n",
            "tensor([[ 0.3828,  0.1303,  0.1957],\n",
            "        [ 0.3828,  0.1303,  0.1957],\n",
            "        [ 0.2079,  0.2185, -0.0029],\n",
            "        [ 0.2079,  0.2185, -0.0029],\n",
            "        [-0.5030,  0.2375, -0.5160]])\n",
            "Embedding variance (should be non-zero):\n",
            "0.1557881236076355\n",
            "Input ASIN: 3931126358, Node index: 68082\n",
            "Sample of scores (first 10):\n",
            "tensor([ 0.5230,  0.5230, -0.1934, -0.1934, -0.0458, -0.0458,  0.1557,  0.1557,\n",
            "        -0.1975, -0.1758])\n",
            "Score variance (should be non-zero):\n",
            "0.14578165113925934\n",
            "Top-k indices: [68083    74 23960 23961  9491]\n",
            "Recommended ASINs: ['3931126900', '0312879237', '0142000760', '1589630424', '0375757805']\n",
            "Recommended Products:\n",
            "              ASIN                                              Title\n",
            "78526   0375757805  A Connecticut Yankee in King Arthur's Court (M...\n",
            "93466   0142000760  Dragon Hunter: Roy Chapman Andrews and the Cen...\n",
            "175734  3931126900  Coast to Coast : Contemporary American Graphic...\n",
            "263934  0312879237  Necroscope: Avengers (Necroscope: E-Branch Tri...\n",
            "322273  1589630424  Across Mongolian Plains: A Naturalists Account...\n"
          ]
        }
      ]
    }
  ]
}