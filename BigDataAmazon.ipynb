{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FredSadeghi/Amazon_CoPurchase_Network_Analysis/blob/main/BigDataAmazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K1oj8EP5ZgpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f83b80-d690-48e7-ec68-93321ff7eba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: python-louvain 0.16\n",
            "Uninstalling python-louvain-0.16:\n",
            "  Successfully uninstalled python-louvain-0.16\n",
            "\u001b[33mWARNING: Skipping community as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting python-louvain\n",
            "  Using cached python_louvain-0.16-py3-none-any.whl\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from python-louvain) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from python-louvain) (2.0.2)\n",
            "Installing collected packages: python-louvain\n",
            "Successfully installed python-louvain-0.16\n",
            "Successfully imported python-louvain for community detection.\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Uninstall and reinstall python-louvain to ensure a clean installation\n",
        "!pip uninstall -y python-louvain community\n",
        "!pip install python-louvain\n",
        "\n",
        "# Try importing the Louvain method\n",
        "try:\n",
        "    from community import best_partition as louvain_best_partition\n",
        "    USE_LOUVAIN = True\n",
        "    print(\"Successfully imported python-louvain for community detection.\")\n",
        "except ImportError as e:\n",
        "    print(f\"Failed to import python-louvain: {e}\")\n",
        "    print(\"Falling back to NetworkX's greedy_modularity_communities for community detection.\")\n",
        "    USE_LOUVAIN = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/Amazon_CoPurchase_Network_Analysis'):\n",
        "    !git clone https://github.com/FredSadeghi/Amazon_CoPurchase_Network_Analysis.git\n",
        "\n",
        "else:\n",
        "    print(\"Repository already cloned. Skipping.\")"
      ],
      "metadata": {
        "id": "ZNa-1qgy9J5m",
        "outputId": "8a9e0d9a-bfea-4032-deb8-c2bad369afdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository already cloned. Skipping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 1: Preprocessing ---\n",
        "# Input and output file paths\n",
        "input_file = '/content/Amazon_CoPurchase_Network_Analysis/amazon-meta.txt.gz'\n",
        "product_output = 'products_cleaned.csv'\n",
        "category_output = 'categories_cleaned.csv'\n",
        "review_output = 'reviews_cleaned.csv'\n",
        "edge_output = 'edges.csv'"
      ],
      "metadata": {
        "id": "s1GXTDjVZxOw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_amazon_data():\n",
        "    \"\"\"Parse raw Amazon metadata into separate CSV files for products, categories, reviews, and weighted edges.\"\"\"\n",
        "    with gzip.open(input_file, 'rt', encoding='latin-1') as f, \\\n",
        "         open(product_output, 'w', newline='', encoding='utf-8') as prod_out, \\\n",
        "         open(category_output, 'w', newline='', encoding='utf-8') as cat_out, \\\n",
        "         open(review_output, 'w', newline='', encoding='utf-8') as rev_out, \\\n",
        "         open(edge_output, 'w', newline='', encoding='utf-8') as edge_out:\n",
        "\n",
        "        product_writer = csv.writer(prod_out)\n",
        "        category_writer = csv.writer(cat_out)\n",
        "        review_writer = csv.writer(rev_out)\n",
        "        edge_writer = csv.writer(edge_out)\n",
        "\n",
        "        # Write headers\n",
        "        product_writer.writerow(['Id', 'ASIN', 'Title', 'Group', 'SalesRank'])\n",
        "        category_writer.writerow(['ASIN', 'CategoryPath'])\n",
        "        review_writer.writerow(['ASIN', 'CustomerID', 'Rating', 'Votes', 'Helpful', 'Sentiment', 'Date'])\n",
        "        edge_writer.writerow(['SourceASIN', 'TargetASIN', 'Weight'])\n",
        "\n",
        "        current = {}\n",
        "        edge_counts = {}  # To track weights for edges\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            # New product entry\n",
        "            if line.startswith(\"Id:\"):\n",
        "                if current.get('ASIN') and current.get('Id'):\n",
        "                    product_writer.writerow([\n",
        "                        current.get('Id'),\n",
        "                        current.get('ASIN'),\n",
        "                        current.get('title', 'Unknown'),\n",
        "                        current.get('group', 'Unknown'),\n",
        "                        current.get('salesrank', '-1')\n",
        "                    ])\n",
        "                    for cat in current.get('categories', []):\n",
        "                        category_writer.writerow([current['ASIN'], cat])\n",
        "                    for review in current.get('reviews', []):\n",
        "                        sentiment = compute_sentiment(review['rating'])\n",
        "                        review_writer.writerow([\n",
        "                            current['ASIN'], review['customer'], review['rating'],\n",
        "                            review['votes'], review['helpful'], sentiment, review['date']\n",
        "                        ])\n",
        "                    for similar_asin in current.get('similar', []):\n",
        "                        # Create undirected edge by sorting ASINs\n",
        "                        edge = tuple(sorted([current['ASIN'], similar_asin]))\n",
        "                        edge_counts[edge] = edge_counts.get(edge, 0) + 1\n",
        "                current = {'categories': [], 'reviews': [], 'similar': []}\n",
        "                current['Id'] = line.split('Id:')[1].strip()\n",
        "\n",
        "            elif line.startswith(\"ASIN:\"):\n",
        "                current['ASIN'] = line.split(\"ASIN:\")[1].strip()\n",
        "\n",
        "            elif 'title:' in line:\n",
        "                match = re.search(r'title:\\s*(.*)', line)\n",
        "                if match:\n",
        "                    current['title'] = match.group(1).strip()\n",
        "\n",
        "            elif 'group:' in line:\n",
        "                match = re.search(r'group:\\s*(.*)', line)\n",
        "                if match:\n",
        "                    current['group'] = match.group(1).strip()\n",
        "\n",
        "            elif 'salesrank:' in line:\n",
        "                match = re.search(r'salesrank:\\s*(.*)', line)\n",
        "                if match:\n",
        "                    current['salesrank'] = match.group(1).strip()\n",
        "\n",
        "            elif line.startswith(\"similar:\"):\n",
        "                parts = line.split()\n",
        "                current['similar'] = parts[2:] if len(parts) > 2 else []\n",
        "\n",
        "            elif line.startswith(\"|\"):\n",
        "                current['categories'].append(line.strip())\n",
        "\n",
        "            elif re.match(r'\\d{4}-\\d{1,2}-\\d{1,2}', line):\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 7:\n",
        "                    review = {\n",
        "                        'date': parts[0],  # e.g., 2000-7-28\n",
        "                        'customer': parts[2],\n",
        "                        'rating': int(parts[4]),\n",
        "                        'votes': int(parts[6]),\n",
        "                        'helpful': int(parts[8])\n",
        "                    }\n",
        "                    current['reviews'].append(review)\n",
        "\n",
        "        # Write the last product\n",
        "        if current.get('ASIN') and current.get('Id'):\n",
        "            product_writer.writerow([\n",
        "                current.get('Id'),\n",
        "                current.get('ASIN'),\n",
        "                current.get('title', 'Unknown'),\n",
        "                current.get('group', 'Unknown'),\n",
        "                current.get('salesrank', '-1')\n",
        "            ])\n",
        "            for cat in current.get('categories', []):\n",
        "                category_writer.writerow([current['ASIN'], cat])\n",
        "            for review in current.get('reviews', []):\n",
        "                sentiment = compute_sentiment(review['rating'])\n",
        "                review_writer.writerow([\n",
        "                    current['ASIN'], review['customer'], review['rating'],\n",
        "                    review['votes'], review['helpful'], sentiment, review['date']\n",
        "                ])\n",
        "            for similar_asin in current.get('similar', []):\n",
        "                edge = tuple(sorted([current['ASIN'], similar_asin]))\n",
        "                edge_counts[edge] = edge_counts.get(edge, 0) + 1\n",
        "\n",
        "        # Write weighted edges\n",
        "        for (source, target), weight in edge_counts.items():\n",
        "            edge_writer.writerow([source, target, weight])"
      ],
      "metadata": {
        "id": "4Y5iTTMiZ8-q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sentiment(rating):\n",
        "    \"\"\"Compute a placeholder sentiment score based on rating (no review text available).\"\"\"\n",
        "    if rating <= 2:\n",
        "        return -1.0  # Negative\n",
        "    elif rating == 3:\n",
        "        return 0.0   # Neutral\n",
        "    else:\n",
        "        return 1.0   # Positive"
      ],
      "metadata": {
        "id": "NM22QwiGaC2-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_data():\n",
        "    \"\"\"Convert CSV files into cleaned, structured pandas DataFrames.\"\"\"\n",
        "    products_df = pd.read_csv(product_output)\n",
        "    categories_df = pd.read_csv(category_output)\n",
        "    reviews_df = pd.read_csv(review_output)\n",
        "\n",
        "    # Clean products DataFrame\n",
        "    products_df['SalesRank'] = pd.to_numeric(products_df['SalesRank'], errors='coerce').fillna(-1).astype(int)\n",
        "    products_df['Title'] = products_df['Title'].fillna('Unknown')\n",
        "    products_df['Group'] = products_df['Group'].fillna('Unknown')\n",
        "\n",
        "    # Parse categories\n",
        "    def parse_category_path(cat_path):\n",
        "        if pd.isna(cat_path):\n",
        "            return []\n",
        "        parts = cat_path.split(\"|\")\n",
        "        return [re.sub(r\"\\[\\d+\\]\", \"\", part).strip() for part in parts if part]\n",
        "\n",
        "    categories_df['CategoryLevels'] = categories_df['CategoryPath'].apply(parse_category_path)\n",
        "    categories_expanded = categories_df.explode('CategoryLevels')\n",
        "\n",
        "    # Aggregate review metrics\n",
        "    review_summary = reviews_df.groupby('ASIN').agg({\n",
        "        'CustomerID': 'count',\n",
        "        'Rating': 'mean',\n",
        "        'Votes': 'sum',\n",
        "        'Helpful': 'sum',\n",
        "        'Sentiment': 'mean'\n",
        "    }).rename(columns={\n",
        "        'CustomerID': 'NumReviews',\n",
        "        'Rating': 'AvgRating',\n",
        "        'Votes': 'TotalVotes',\n",
        "        'Helpful': 'TotalHelpful',\n",
        "        'Sentiment': 'AvgSentiment'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Join with products\n",
        "    products_enriched = products_df.merge(review_summary, on='ASIN', how='left')\n",
        "    products_enriched = products_enriched.fillna({\n",
        "        'NumReviews': 0, 'AvgRating': 0.0, 'TotalVotes': 0, 'TotalHelpful': 0, 'AvgSentiment': 0.0\n",
        "    })\n",
        "\n",
        "    # Save final cleaned data\n",
        "    products_enriched.to_csv('products_enriched.csv', index=False)\n",
        "    categories_expanded.to_csv('categories_expanded.csv', index=False)\n",
        "    reviews_df.to_csv('reviews_processed.csv', index=False)\n",
        "\n",
        "    return products_enriched, categories_expanded, reviews_df"
      ],
      "metadata": {
        "id": "Ikzj-sKiaDhz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 2: Graph Construction & Analysis ---\n",
        "# Output file paths\n",
        "edges_file = 'edges.csv'\n",
        "products_file = 'products_enriched.csv'\n",
        "graph_metrics_file = 'graph_metrics.csv'\n",
        "influential_nodes_file = 'influential_nodes.csv'\n",
        "degree_dist_plot = 'degree_distribution.png'\n",
        "graph_plot = 'copurchase_graph.png'"
      ],
      "metadata": {
        "id": "EIbMbIHtQXnM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_copurchase_graph(edges_df):\n",
        "    \"\"\"Construct an undirected graph with weighted edges from co-purchasing relationships.\"\"\"\n",
        "    G = nx.Graph()  # Use undirected graph\n",
        "    for _, row in edges_df.iterrows():\n",
        "        source = row['SourceASIN']\n",
        "        target = row['TargetASIN']\n",
        "        weight = row['Weight']\n",
        "        G.add_edge(source, target, weight=weight)\n",
        "    print(f\"Graph constructed with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
        "    return G"
      ],
      "metadata": {
        "id": "kocCgdySQjCL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_node_attributes(G, products_df):\n",
        "    \"\"\"Add product attributes to graph nodes.\"\"\"\n",
        "    if products_df is not None:\n",
        "        for _, row in products_df.iterrows():\n",
        "            asin = row['ASIN']\n",
        "            if asin in G.nodes:\n",
        "                G.nodes[asin]['Title'] = row.get('Title', 'Unknown')\n",
        "                G.nodes[asin]['Group'] = row.get('Group', 'Unknown')\n",
        "                G.nodes[asin]['SalesRank'] = row.get('SalesRank', -1)\n",
        "                G.nodes[asin]['AvgRating'] = row.get('AvgRating', 0.0)\n",
        "                G.nodes[asin]['NumReviews'] = row.get('NumReviews', 0)\n",
        "                G.nodes[asin]['AvgSentiment'] = row.get('AvgSentiment', 0.0)"
      ],
      "metadata": {
        "id": "IOhweSq_Qls4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_graph_structure(G):\n",
        "    \"\"\"Analyze the graph's structure and compute basic metrics.\"\"\"\n",
        "    metrics = {\n",
        "        'num_nodes': G.number_of_nodes(),\n",
        "        'num_edges': G.number_of_edges(),\n",
        "        'density': nx.density(G),\n",
        "        'avg_clustering': nx.average_clustering(G),\n",
        "        'num_components': nx.number_connected_components(G)  # For undirected graph\n",
        "    }\n",
        "    degrees = [d for _, d in G.degree()]\n",
        "    metrics['avg_degree'] = np.mean(degrees) if degrees else 0\n",
        "\n",
        "    print(\"Graph Metrics:\")\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    return metrics, degrees"
      ],
      "metadata": {
        "id": "UguH1UuKQuoY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_degree_distribution(degrees):\n",
        "    \"\"\"Plot the degree distribution with a power-law fit.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    counts = Counter(degrees)\n",
        "    plt.scatter(counts.keys(), counts.values(), color='blue', alpha=0.5, label='Degree')\n",
        "    # Fit power-law\n",
        "    degrees = np.array(list(counts.keys()))\n",
        "    frequencies = np.array(list(counts.values()))\n",
        "    mask = (degrees > 0) & (frequencies > 0)  # Avoid log(0)\n",
        "    log_degrees = np.log10(degrees[mask])\n",
        "    log_freq = np.log10(frequencies[mask])\n",
        "    if len(log_degrees) > 1:  # Need at least 2 points to fit\n",
        "        coeffs = np.polyfit(log_degrees, log_freq, 1)\n",
        "        plt.plot(degrees, 10**(coeffs[1] + coeffs[0] * np.log10(degrees)), 'k--',\n",
        "                 label=f'Power-law fit (γ={-coeffs[0]:.2f})')\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Degree')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Degree Distribution of Co-Purchasing Network')\n",
        "    plt.legend()\n",
        "    plt.savefig(degree_dist_plot)\n",
        "    plt.close()\n",
        "    print(f\"Degree distribution plot saved to {degree_dist_plot}\")"
      ],
      "metadata": {
        "id": "-AZ5jTyJQw58"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_influential_nodes(G):\n",
        "    \"\"\"Compute centrality metrics to identify influential nodes.\"\"\"\n",
        "    degree_centrality = nx.degree_centrality(G)\n",
        "    betweenness_centrality = nx.betweenness_centrality(G, k=100)  # Reduced k for speed\n",
        "    pagerank = nx.pagerank(G, weight='weight')  # Use weighted PageRank\n",
        "\n",
        "    centrality_df = pd.DataFrame({\n",
        "        'ASIN': list(degree_centrality.keys()),\n",
        "        'DegreeCentrality': list(degree_centrality.values()),\n",
        "        'BetweennessCentrality': [betweenness_centrality.get(node, 0) for node in degree_centrality],\n",
        "        'PageRank': [pagerank.get(node, 0) for node in degree_centrality]\n",
        "    })\n",
        "    influential_nodes = centrality_df.sort_values(by='PageRank', ascending=False).head(10)\n",
        "\n",
        "    return centrality_df, influential_nodes"
      ],
      "metadata": {
        "id": "uDzy8rDAQyxR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_communities(G):\n",
        "    \"\"\"Detect communities using either Louvain (if available) or NetworkX's greedy modularity.\"\"\"\n",
        "    if USE_LOUVAIN:\n",
        "        # Use python-louvain's best_partition\n",
        "        partition = louvain_best_partition(G)\n",
        "        for node, comm in partition.items():\n",
        "            G.nodes[node]['community'] = comm\n",
        "        num_communities = len(set(partition.values()))\n",
        "        print(f\"Detected {num_communities} communities using Louvain method\")\n",
        "    else:\n",
        "        # Fallback to NetworkX's greedy_modularity_communities\n",
        "        communities = list(nx.algorithms.community.greedy_modularity_communities(G))\n",
        "        partition = {}\n",
        "        for comm_id, comm_nodes in enumerate(communities):\n",
        "            for node in comm_nodes:\n",
        "                partition[node] = comm_id\n",
        "                G.nodes[node]['community'] = comm_id\n",
        "        num_communities = len(communities)\n",
        "        print(f\"Detected {num_communities} communities using NetworkX greedy modularity\")\n",
        "    return partition"
      ],
      "metadata": {
        "id": "zmariHfPiBo9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_by_category(G, products_df):\n",
        "    \"\"\"Analyze edges within and between product groups.\"\"\"\n",
        "    asin_to_group = dict(zip(products_df['ASIN'], products_df['Group']))\n",
        "    group_edges = {'within': {}, 'between': {}}\n",
        "    for u, v in G.edges():\n",
        "        group_u = asin_to_group.get(u, 'Unknown')\n",
        "        group_v = asin_to_group.get(v, 'Unknown')\n",
        "        if group_u == group_v:\n",
        "            group_edges['within'][group_u] = group_edges['within'].get(group_u, 0) + 1\n",
        "        else:\n",
        "            edge = tuple(sorted([group_u, group_v]))\n",
        "            group_edges['between'][edge] = group_edges['between'].get(edge, 0) + 1\n",
        "    print(\"Edges within groups:\", group_edges['within'])\n",
        "    print(\"Edges between groups:\", group_edges['between'])"
      ],
      "metadata": {
        "id": "r-pnKPJKiEGe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment_centrality(centrality_df, products_df):\n",
        "    \"\"\"Analyze correlation between centrality and sentiment/reviews.\"\"\"\n",
        "    merged_df = centrality_df.merge(products_df[['ASIN', 'AvgSentiment', 'NumReviews']], on='ASIN')\n",
        "    corr_pagerank_sentiment = merged_df['PageRank'].corr(merged_df['AvgSentiment'])\n",
        "    corr_pagerank_reviews = merged_df['PageRank'].corr(merged_df['NumReviews'])\n",
        "    print(f\"Correlation between PageRank and AvgSentiment: {corr_pagerank_sentiment:.3f}\")\n",
        "    print(f\"Correlation between PageRank and NumReviews: {corr_pagerank_reviews:.3f}\")"
      ],
      "metadata": {
        "id": "9witFPKXiGTc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_graph(G):\n",
        "    \"\"\"Visualize a sampled subgraph with enhanced features.\"\"\"\n",
        "    top_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:50]\n",
        "    top_nodes = [node for node, _ in top_nodes]\n",
        "    subgraph = G.subgraph(top_nodes)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    pos = nx.spring_layout(subgraph)\n",
        "    # Color by group\n",
        "    groups = [subgraph.nodes[node].get('Group', 'Unknown') for node in subgraph.nodes()]\n",
        "    unique_groups = list(set(groups))\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_groups)))\n",
        "    group_to_color = dict(zip(unique_groups, colors))\n",
        "    node_colors = [group_to_color[group] for group in groups]\n",
        "    # Size by degree\n",
        "    degrees = [subgraph.degree(node) * 10 for node in subgraph.nodes()]\n",
        "    # Edge weights\n",
        "    edge_weights = [subgraph[u][v]['weight'] for u, v in subgraph.edges()]\n",
        "    nx.draw(subgraph, pos, node_size=degrees, node_color=node_colors, edge_color='gray',\n",
        "            width=edge_weights, with_labels=True, font_size=8)\n",
        "    plt.title('Co-Purchasing Network (Sampled Subgraph)')\n",
        "    plt.savefig(graph_plot)\n",
        "    plt.close()\n",
        "    print(f\"Graph visualization saved to {graph_plot}\")"
      ],
      "metadata": {
        "id": "KITW_oe0Q1Mr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_graph_analysis():\n",
        "    \"\"\"Execute graph construction and analysis using preprocessed data.\"\"\"\n",
        "    if not os.path.exists(edges_file):\n",
        "        raise FileNotFoundError(f\"Edges file {edges_file} not found. Preprocessing must complete first.\")\n",
        "    edges_df = pd.read_csv(edges_file)\n",
        "\n",
        "    products_df = None\n",
        "    if os.path.exists(products_file):\n",
        "        products_df = pd.read_csv(products_file)\n",
        "        print(f\"Loaded products data with {len(products_df)} records\")\n",
        "\n",
        "    print(\"Building co-purchasing graph...\")\n",
        "    start = time.time()\n",
        "    G = build_copurchase_graph(edges_df)\n",
        "    print(f\"Graph construction took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    if products_df is not None:\n",
        "        print(\"Adding node attributes...\")\n",
        "        start = time.time()\n",
        "        add_node_attributes(G, products_df)\n",
        "        print(f\"Adding attributes took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Analyzing graph structure...\")\n",
        "    start = time.time()\n",
        "    metrics, degrees = analyze_graph_structure(G)\n",
        "    print(f\"Structure analysis took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Plotting degree distribution...\")\n",
        "    start = time.time()\n",
        "    plot_degree_distribution(degrees)\n",
        "    print(f\"Degree plotting took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Identifying influential nodes...\")\n",
        "    start = time.time()\n",
        "    centrality_df, influential_nodes = find_influential_nodes(G)\n",
        "    print(f\"Influential nodes computation took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Detecting communities...\")\n",
        "    start = time.time()\n",
        "    partition = detect_communities(G)\n",
        "    print(f\"Community detection took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Analyzing by category...\")\n",
        "    start = time.time()\n",
        "    analyze_by_category(G, products_df)\n",
        "    print(f\"Category analysis took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Analyzing sentiment and reviews...\")\n",
        "    start = time.time()\n",
        "    analyze_sentiment_centrality(centrality_df, products_df)\n",
        "    print(f\"Sentiment analysis took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    metrics_df = pd.DataFrame([metrics])\n",
        "    metrics_df.to_csv(graph_metrics_file, index=False)\n",
        "    print(f\"Graph metrics saved to {graph_metrics_file}\")\n",
        "\n",
        "    centrality_df.to_csv(influential_nodes_file, index=False)\n",
        "    print(f\"Influential nodes saved to {influential_nodes_file}\")\n",
        "    print(\"\\nTop 10 Influential Nodes (by PageRank):\")\n",
        "    print(influential_nodes)\n",
        "\n",
        "    print(\"Visualizing graph...\")\n",
        "    start = time.time()\n",
        "    visualize_graph(G)\n",
        "    print(f\"Graph visualization took {time.time() - start:.2f} seconds\")"
      ],
      "metadata": {
        "id": "huUsYWHuQ6cx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Part 1: Preprocessing Amazon Metadata\")\n",
        "    start = time.time()\n",
        "    print(\"Parsing Amazon metadata...\")\n",
        "    parse_start = time.time()\n",
        "    parse_amazon_data()\n",
        "    print(f\"Parsing took {time.time() - parse_start:.2f} seconds\")\n",
        "\n",
        "    print(\"Converting and cleaning data...\")\n",
        "    convert_start = time.time()\n",
        "    products_enriched, categories_expanded, reviews_processed = convert_data()\n",
        "    print(f\"Converting took {time.time() - convert_start:.2f} seconds\")\n",
        "    print(\"Done. Cleaned data saved to products_enriched.csv, categories_expanded.csv, reviews_processed.csv, and edges.csv\")\n",
        "    print(f\"Total preprocessing took {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"\\nPart 2: Graph Construction and Analysis\")\n",
        "    start = time.time()\n",
        "    perform_graph_analysis()\n",
        "    print(f\"Graph analysis took {time.time() - start:.2f} seconds\")\n",
        "    print(\"Graph analysis complete.\")"
      ],
      "metadata": {
        "id": "pdbdCFv-Q87C",
        "outputId": "79a1a88c-321b-48a0-fec8-886c62006984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part 1: Preprocessing Amazon Metadata\n",
            "Parsing Amazon metadata...\n",
            "Parsing took 116.80 seconds\n",
            "Converting and cleaning data...\n",
            "Converting took 182.06 seconds\n",
            "Done. Cleaned data saved to products_enriched.csv, categories_expanded.csv, reviews_processed.csv, and edges.csv\n",
            "Total preprocessing took 298.86 seconds\n",
            "\n",
            "Part 2: Graph Construction and Analysis\n",
            "Loaded products data with 548552 records\n",
            "Building co-purchasing graph...\n",
            "Graph constructed with 554789 nodes and 1545228 edges\n",
            "Graph construction took 101.43 seconds\n",
            "Adding node attributes...\n",
            "Adding attributes took 40.60 seconds\n",
            "Analyzing graph structure...\n",
            "Graph Metrics:\n",
            "num_nodes: 554789\n",
            "num_edges: 1545228\n",
            "density: 1.0040784922418376e-05\n",
            "avg_clustering: 0.3013179862039754\n",
            "num_components: 5614\n",
            "avg_degree: 5.570506985538646\n",
            "Structure analysis took 26.49 seconds\n",
            "Plotting degree distribution...\n",
            "Degree distribution plot saved to degree_distribution.png\n",
            "Degree plotting took 0.67 seconds\n",
            "Identifying influential nodes...\n",
            "Influential nodes computation took 1021.30 seconds\n",
            "Detecting communities...\n",
            "Detected 5851 communities using Louvain method\n",
            "Community detection took 308.75 seconds\n",
            "Analyzing by category...\n",
            "Edges within groups: {'Book': 741124, 'Music': 147483, 'DVD': 39878, 'Video': 16724, 'Toy': 2}\n",
            "Edges between groups: {('Book', 'Unknown'): 399868, ('Music', 'Unknown'): 94804, ('Book', 'DVD'): 3394, ('Book', 'Video'): 2261, ('DVD', 'Video'): 31662, ('DVD', 'Unknown'): 33944, ('DVD', 'Music'): 2760, ('Unknown', 'Video'): 28668, ('Music', 'Video'): 918, ('Book', 'Music'): 1676, ('Toy', 'Unknown'): 26, ('Music', 'Toy'): 12, ('Sports', 'Unknown'): 1, ('Sports', 'Video'): 1, ('Book', 'Software'): 3, ('Music', 'Software'): 1, ('Book', 'Video Games'): 3, ('Unknown', 'Video Games'): 2, ('Software', 'Unknown'): 7, ('Baby Product', 'Unknown'): 5, ('Book', 'Toy'): 1}\n",
            "Category analysis took 4.23 seconds\n",
            "Analyzing sentiment and reviews...\n",
            "Correlation between PageRank and AvgSentiment: 0.086\n",
            "Correlation between PageRank and NumReviews: 0.040\n",
            "Sentiment analysis took 0.85 seconds\n",
            "Graph metrics saved to graph_metrics.csv\n",
            "Influential nodes saved to influential_nodes.csv\n",
            "\n",
            "Top 10 Influential Nodes (by PageRank):\n",
            "             ASIN  DegreeCentrality  BetweennessCentrality  PageRank\n",
            "3396   B0002IVN9W          0.000543               0.006293  0.000084\n",
            "1008   B00008LDNZ          0.000999               0.006820  0.000078\n",
            "5575   B00023P4I8          0.000844               0.007685  0.000075\n",
            "6574   0670033375          0.000469               0.031333  0.000066\n",
            "5030   0890420254          0.000584               0.005972  0.000065\n",
            "623    0679454438          0.000508               0.006661  0.000059\n",
            "6750   1557987912          0.000465               0.015801  0.000057\n",
            "19054  1582701148          0.000429               0.010039  0.000056\n",
            "8524   0393050572          0.000382               0.012766  0.000052\n",
            "11976  1594200246          0.000409               0.003646  0.000051\n",
            "Visualizing graph...\n",
            "Graph visualization saved to copurchase_graph.png\n",
            "Graph visualization took 1.57 seconds\n",
            "Graph analysis took 1514.71 seconds\n",
            "Graph analysis complete.\n",
            "Part 1: Preprocessing Amazon Metadata\n",
            "Parsing Amazon metadata...\n",
            "Parsing took 101.17 seconds\n",
            "Converting and cleaning data...\n",
            "Converting took 184.12 seconds\n",
            "Done. Cleaned data saved to products_enriched.csv, categories_expanded.csv, reviews_processed.csv, and edges.csv\n",
            "Total preprocessing took 287.06 seconds\n",
            "\n",
            "Part 2: Graph Construction and Analysis\n",
            "Loaded products data with 548552 records\n",
            "Building co-purchasing graph...\n"
          ]
        }
      ]
    }
  ]
}